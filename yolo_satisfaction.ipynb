{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLOv8 Pose + Face — Satisfaction Monitoring\n",
        "\n",
        "This notebook contains the **full pipeline** you provided, split into runnable cells with explanatory markdown describing design choices and operation. Run cells in order. The notebook is intended for **local execution** (it opens OpenCV windows) and may download model weights the first time it's run.\n",
        "\n",
        "**What this notebook contains:**\n",
        "- setup (install + imports)\n",
        "- model download / loading\n",
        "- helper functions and scoring logic (posture & face)\n",
        "- client-zone selection UI\n",
        "- main processing loop as a callable function\n",
        "- insight generation using OpenRouter/OpenAI client (optional)\n",
        "\n",
        "**Notes / choices:**\n",
        "- I kept your original scoring logic intact but added comments and small robustness checks.\n",
        "- The face detector and emotion models are downloaded if missing. For fast testing you can swap to lighter models.\n",
        "- The processing loop is in a function so you can stop it by interrupting the kernel.\n",
        "- Visual output uses OpenCV windows for interactive selection and real-time view; inline frames are not used to avoid flooding notebook output.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet ultralytics opencv-python tqdm timm torch torchvision python-dotenv openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Moetez\\Desktop\\CV Projects\\AI vision hackathon\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Imports and environment\n",
        "import time, collections, math, os, threading, textwrap\n",
        "import numpy as np\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import torch, timm\n",
        "from openai import OpenAI\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yolov8n-face-lindevs.pt already exists, skipping download\n",
            "enet_b0_8_best_afew.pt already exists, skipping download\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# CONFIG - change these paths to your local setup if desired\n",
        "VIDEO_PATH = './testcases/siu.mp4'  # change to your video file\n",
        "TINY_FACE_URL  = 'https://github.com/lindevs/yolov8-face/releases/download/v1.0.0/yolov8n-face-lindevs.pt'\n",
        "TINY_FACE_PATH = 'yolov8n-face-lindevs.pt'\n",
        "EMOTION_MODEL_URL = 'https://github.com/sb-ai-lab/EmotiEffLib/raw/main/models/affectnet_emotions/enet_b0_8_best_afew.pt'\n",
        "EMOTION_MODEL_PATH = 'enet_b0_8_best_afew.pt'\n",
        "\n",
        "def _download(url, dst):\n",
        "    if os.path.exists(dst):\n",
        "        print(f'{dst} already exists, skipping download')\n",
        "        return\n",
        "    print(f'Downloading {dst} ...')\n",
        "    with tqdm(unit='B', unit_scale=True, desc=os.path.basename(dst)) as t:\n",
        "        def _reporthook(b, bs, ts):\n",
        "            if ts != -1:\n",
        "                t.total = ts\n",
        "            t.update(bs)\n",
        "        urllib.request.urlretrieve(url, dst, reporthook=_reporthook)\n",
        "    print('Done!')\n",
        "\n",
        "# Download models if missing (comment out if you already have them)\n",
        "_download(TINY_FACE_URL, TINY_FACE_PATH)\n",
        "_download(EMOTION_MODEL_URL, EMOTION_MODEL_PATH)\n",
        "\n",
        "# Open video capture (will error later if path invalid)\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "if not cap.isOpened():\n",
        "    print('Warning: Could not open video file. Make sure VIDEO_PATH is correct.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model loading\n",
        "\n",
        "Load YOLO pose model, tiny face detector and the emotion classifier. The emotion checkpoint may be either a state dictionary or a serialized model object — we attempt to load robustly and fall back gracefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading YOLO pose model -- this may take a while...\n",
            "Loading tiny face model...\n",
            "Loading emotion model architecture...\n",
            "Model loading complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Moetez\\AppData\\Local\\Temp\\ipykernel_6404\\4131876235.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(EMOTION_MODEL_PATH, map_location='cpu')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('Loading YOLO pose model -- this may take a while...')\n",
        "model = YOLO('yolov8x-pose.pt')  # replace if you want a smaller model for testing\n",
        "print('Loading tiny face model...')\n",
        "face_model = YOLO(TINY_FACE_PATH)\n",
        "\n",
        "print('Loading emotion model architecture...')\n",
        "emotion_model = timm.create_model('efficientnet_b0', num_classes=8, pretrained=False)\n",
        "\n",
        "# Try robust loading of the checkpoint\n",
        "try:\n",
        "    ckpt = torch.load(EMOTION_MODEL_PATH, map_location='cpu')\n",
        "    if isinstance(ckpt, dict) and 'state_dict' in ckpt:\n",
        "        # many checkpoints store weights under 'state_dict'\n",
        "        state = ckpt['state_dict']\n",
        "        # fix any 'module.' prefixes if present\n",
        "        new_state = {}\n",
        "        for k, v in state.items():\n",
        "            nk = k.replace('module.', '') if k.startswith('module.') else k\n",
        "            # adapt classification head key if necessary\n",
        "            new_state[nk] = v\n",
        "        emotion_model.load_state_dict(new_state, strict=False)\n",
        "    elif isinstance(ckpt, dict):\n",
        "        emotion_model.load_state_dict(ckpt, strict=False)\n",
        "    else:\n",
        "        # checkpoint appears to be a full model object\n",
        "        emotion_model = ckpt\n",
        "except Exception as e:\n",
        "    print('Warning: fallback loading for emotion model failed:', e)\n",
        "    try:\n",
        "        emotion_model = torch.load(EMOTION_MODEL_PATH, map_location='cpu', weights_only=False)\n",
        "    except Exception as e2:\n",
        "        print('Final fallback failed. Emotion model may not be usable:', e2)\n",
        "\n",
        "emotion_model.eval()\n",
        "print('Model loading complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers and scoring functions\n",
        "\n",
        "This cell contains the posture/face scoring functions. I preserved your core logic but added defensive checks and small clarifying comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "HISTORY_LEN = 20\n",
        "TORSO_ACTIVITY_THRESHOLD = 0.0025\n",
        "ARM_ACTIVITY_THRESHOLD = 0.005\n",
        "MIN_SHOULDER_WIDTH_FRAC = 0.10\n",
        "SCORE_INTERVAL = 1.0  # seconds for scoring\n",
        "\n",
        "COCO_CONNECTIONS = [\n",
        "    (0,1),(0,2),(1,3),(2,4),(0,5),(0,6),(5,7),(7,9),(6,8),(8,10),\n",
        "    (5,6),(5,11),(6,12),(11,13),(13,15),(12,14),(14,16),(11,12)\n",
        "]\n",
        "\n",
        "def _angle_between_deg(v1, v2):\n",
        "    n1 = np.linalg.norm(v1); n2 = np.linalg.norm(v2)\n",
        "    if n1 == 0 or n2 == 0: return 0.0\n",
        "    c = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
        "    return math.degrees(math.acos(c))\n",
        "\n",
        "def _clamp01(x): return max(0.0, min(1.0, x))\n",
        "def _map_to_01(v, a, b):\n",
        "    if b == a: return 0.0\n",
        "    return _clamp01((v - a) / (b - a))\n",
        "\n",
        "def compute_satisfaction_score(history, fw, fh, prev_score=None, alpha=0.6):\n",
        "    # returns (label, score, diagnostics)\n",
        "    if len(history) < 3: return None, 0, {}\n",
        "    pts_now = history[-1]; pts_prev = history[-2]; pts_old = history[-3]\n",
        "    if pts_now.shape[0] < 17: return None, 0, {}\n",
        "\n",
        "    disp1 = np.linalg.norm(pts_now - pts_prev, axis=1)\n",
        "    disp2 = np.linalg.norm(pts_prev - pts_old, axis=1)\n",
        "    activity = float((disp1.mean() + disp2.mean()) / 2.0)\n",
        "\n",
        "    L_SH, R_SH = 5,6\n",
        "    L_HIP, R_HIP = 11,12\n",
        "    NOSE = 0\n",
        "    L_ELB, R_ELB = 7,8\n",
        "    L_WRIST, R_WRIST = 9,10\n",
        "\n",
        "    shoulders = (pts_now[L_SH] + pts_now[R_SH]) / 2.0\n",
        "    hips      = (pts_now[L_HIP] + pts_now[R_HIP]) / 2.0\n",
        "    nose      = pts_now[NOSE]\n",
        "\n",
        "    torso_vec = hips - shoulders\n",
        "    torso_len = np.linalg.norm(torso_vec) + 1e-6\n",
        "    torso_dir = torso_vec / torso_len\n",
        "    vertical  = np.array([0.0, 1.0])\n",
        "    torso_angle = min(_angle_between_deg(torso_dir, vertical), 180 - _angle_between_deg(torso_dir, vertical))\n",
        "\n",
        "    head_vec = nose - shoulders\n",
        "    head_len = np.linalg.norm(head_vec) + 1e-6\n",
        "    head_dir = head_vec / head_len\n",
        "    head_torso_angle = min(_angle_between_deg(head_dir, torso_dir), 180 - _angle_between_deg(head_dir, torso_dir))\n",
        "\n",
        "    sh_ys = abs(pts_now[L_SH][1] - pts_now[R_SH][1])\n",
        "    shoulder_sym = sh_ys * fh / (torso_len * fh + 1e-6)\n",
        "\n",
        "    shoulder_width = np.linalg.norm(pts_now[L_SH] - pts_now[R_SH]) + 1e-6\n",
        "    wrist_dist = np.linalg.norm(pts_now[L_WRIST] - pts_now[R_WRIST])\n",
        "    arm_openness = _clamp01((wrist_dist / shoulder_width) / 2.5)\n",
        "\n",
        "    wrist_to_nose = min(np.linalg.norm(pts_now[L_WRIST] - nose),\n",
        "                        np.linalg.norm(pts_now[R_WRIST] - nose))\n",
        "    hands_face = _map_to_01(wrist_to_nose, 0.01, 0.20)\n",
        "\n",
        "    crossed_arms_penalty = 0.0\n",
        "    try:\n",
        "        if all(pts_now[i].any() for i in [L_ELB, R_ELB, L_WRIST, R_WRIST, L_SH, R_SH, L_HIP, R_HIP]):\n",
        "            if (pts_now[L_WRIST][0] > pts_now[R_SH][0] and pts_now[R_WRIST][0] < pts_now[L_SH][0]) or                (pts_now[L_WRIST][0] > pts_now[R_ELB][0] and pts_now[R_WRIST][0] < pts_now[L_ELB][0]):\n",
        "                chest_y_min = min(pts_now[L_SH][1], pts_now[R_SH][1])\n",
        "                chest_y_max = max(pts_now[L_HIP][1], pts_now[R_HIP][1])\n",
        "                if chest_y_min < pts_now[L_WRIST][1] < chest_y_max and chest_y_min < pts_now[R_WRIST][1] < chest_y_max:\n",
        "                    left_arm_vec = pts_now[L_WRIST] - pts_now[L_ELB]\n",
        "                    right_arm_vec = pts_now[R_WRIST] - pts_now[R_ELB]\n",
        "                    cross_prod = left_arm_vec[0] * right_arm_vec[1] - left_arm_vec[1] * right_arm_vec[0]\n",
        "                    if abs(cross_prod) > 0.01:\n",
        "                        crossed_arms_penalty = 0.50\n",
        "    except Exception:\n",
        "        crossed_arms_penalty = 0.0\n",
        "\n",
        "    activity_score = _map_to_01(activity, 0.0008, 0.018)\n",
        "    upright_score = 1.0 if torso_angle <= 10 else 0.0 if torso_angle >= 40 else 1.0 - ((torso_angle-10)/(40-10))\n",
        "    head_align_score = _clamp01(1.0 - (head_torso_angle/40.0))\n",
        "    shoulder_sym_score = 1.0 - _clamp01(shoulder_sym*3.0)\n",
        "    hands_open_score = hands_face\n",
        "    arm_open_score = arm_openness\n",
        "\n",
        "    combined = (0.40*upright_score + 0.20*head_align_score + 0.10*activity_score +\n",
        "                0.10*hands_open_score + 0.10*arm_open_score + 0.10*shoulder_sym_score)\n",
        "    combined -= crossed_arms_penalty\n",
        "\n",
        "    harsh_penalty = 0.0\n",
        "    if activity > 0.018:\n",
        "        harsh_penalty = 0.30\n",
        "    combined -= harsh_penalty\n",
        "\n",
        "    if arm_openness < 0.3:\n",
        "        combined -= 0.20\n",
        "\n",
        "    if arm_openness > 0.7 and upright_score > 0.8 and activity < 0.005:\n",
        "        combined += 0.20\n",
        "\n",
        "    combined = _clamp01(combined)\n",
        "\n",
        "    if prev_score is not None:\n",
        "        combined = alpha*combined + (1-alpha)*(prev_score/100.0)\n",
        "\n",
        "    score = int(combined*100)\n",
        "\n",
        "    if harsh_penalty > 0:\n",
        "        label = 'angry'\n",
        "    elif crossed_arms_penalty > 0:\n",
        "        label = 'bored'\n",
        "    elif arm_openness < 0.3:\n",
        "        label = 'uncomfortable'\n",
        "    elif arm_openness > 0.7 and upright_score > 0.8:\n",
        "        label = 'comfortable'\n",
        "    else:\n",
        "        label = 'satisfied' if score >= 70 else 'neutral' if score >= 45 else 'dissatisfied'\n",
        "\n",
        "    return label, score, {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Face detection helper and drawing utilities\n",
        "\n",
        "We throttle face detection to every 0.5s to save compute and re-use a cached crop when appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "last_face_check = 0.0\n",
        "cached_face_box = None\n",
        "cached_face_crop = None\n",
        "\n",
        "def box_overlap(box1, box2):\n",
        "    ix1 = max(box1[0], box2[0])\n",
        "    iy1 = max(box1[1], box2[1])\n",
        "    ix2 = min(box1[2], box2[2])\n",
        "    iy2 = min(box1[3], box2[3])\n",
        "    ia  = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n",
        "    a1  = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    return ia / a1 if a1 > 0 else 0.0\n",
        "\n",
        "def get_face_for_person(person_bbox, now, frame):\n",
        "    global last_face_check, cached_face_box, cached_face_crop\n",
        "    if now - last_face_check < 0.5:\n",
        "        if cached_face_box and box_overlap(person_bbox, cached_face_box) > 0.4:\n",
        "            return cached_face_box, cached_face_crop\n",
        "\n",
        "    x1, y1, x2, y2 = person_bbox\n",
        "    h, w = frame.shape[:2]\n",
        "    x1 = max(0, min(w-1, x1)); x2 = max(0, min(w, x2))\n",
        "    y1 = max(0, min(h-1, y1)); y2 = max(0, min(h, y2))\n",
        "    crop = frame[y1:y2, x1:x2]\n",
        "    if crop.size == 0:\n",
        "        return None, None\n",
        "\n",
        "    results = face_model(crop, verbose=False, conf=0.25)\n",
        "    if results[0].boxes is None or len(results[0].boxes) == 0:\n",
        "        cached_face_box = None; cached_face_crop = None\n",
        "        return None, None\n",
        "\n",
        "    best = results[0].boxes[0]\n",
        "    fx1, fy1, fx2, fy2 = map(int, best.xyxy[0].tolist())\n",
        "    fx1 += x1; fy1 += y1; fx2 += x1; fy2 += y1\n",
        "    face_box = (fx1, fy1, fx2, fy2)\n",
        "\n",
        "    crop_face_y1 = max(0, fy1 - y1)\n",
        "    crop_face_y2 = min(crop.shape[0], fy2 - y1)\n",
        "    crop_face_x1 = max(0, fx1 - x1)\n",
        "    crop_face_x2 = min(crop.shape[1], fx2 - x1)\n",
        "    face_crop = crop[crop_face_y1:crop_face_y2, crop_face_x1:crop_face_x2]\n",
        "    if face_crop.size == 0:\n",
        "        face_crop = None\n",
        "\n",
        "    cached_face_box = face_box\n",
        "    cached_face_crop = face_crop\n",
        "    last_face_check = now\n",
        "    return face_box, face_crop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Face expression scoring\n",
        "\n",
        "The emotion model outputs 8 classes (assumed mapping). We compute a simplified 'face satisfaction score' from a weighted sum of relevant probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def compute_face_expression_score(face_crop, prev_score=None, alpha=0.6):\n",
        "    if face_crop is None or face_crop.size == 0:\n",
        "        return None, 0, {}\n",
        "    resized = cv2.resize(face_crop, (224, 224))\n",
        "    normalized = resized / 255.0\n",
        "    tensor = torch.tensor(normalized, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    tensor = (tensor - mean) / std\n",
        "    with torch.no_grad():\n",
        "        output = emotion_model(tensor)\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "    # class indices (assumed): 0:anger,1:contempt,2:disgust,3:fear,4:happy,5:neutral,6:sad,7:surprise\n",
        "    happy_prob = probs[0,4].item()\n",
        "    neutral_prob = probs[0,5].item()\n",
        "    surprise_prob = probs[0,7].item()\n",
        "    score = happy_prob * 100 + neutral_prob * 50 + surprise_prob * 30\n",
        "    score = min(score, 100.0)\n",
        "    if prev_score is not None:\n",
        "        score = alpha * score + (1 - alpha) * prev_score\n",
        "    label = 'satisfied' if score >= 70 else 'neutral' if score >= 45 else 'dissatisfied'\n",
        "    return label, int(score), {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select client zone (interactive)\n",
        "\n",
        "Run the next cell and draw a rectangle around the area where clients stand using the mouse. Press 'q' to accept the selection. If the video can't be opened, fix `VIDEO_PATH` first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Draw a rectangle around the client area. Press q to accept.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     30\u001b[39m     cv2.imshow(\u001b[33m'\u001b[39m\u001b[33mSelect Client Zone\u001b[39m\u001b[33m'\u001b[39m, frame_copy)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m & \u001b[32m0xFF\u001b[39m == \u001b[38;5;28mord\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m rect_drawn:\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     33\u001b[39m cv2.destroyWindow(\u001b[33m'\u001b[39m\u001b[33mSelect Client Zone\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "drawing = False\n",
        "ix, iy = -1, -1\n",
        "rect_drawn = False\n",
        "client_box = None\n",
        "\n",
        "def draw_rectangle(event, x, y, flags, param):\n",
        "    global ix, iy, drawing, frame_copy, rect_drawn, client_box\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:\n",
        "        drawing = True; ix, iy = x, y\n",
        "    elif event == cv2.EVENT_MOUSEMOVE:\n",
        "        if drawing:\n",
        "            img = frame_copy.copy()\n",
        "            cv2.rectangle(img, (ix, iy), (x, y), (0, 255, 0), 2)\n",
        "            cv2.imshow('Select Client Zone', img)\n",
        "    elif event == cv2.EVENT_LBUTTONUP:\n",
        "        drawing = False\n",
        "        cv2.rectangle(frame_copy, (ix, iy), (x, y), (0, 255, 0), 2)\n",
        "        cv2.imshow('Select Client Zone', frame_copy)\n",
        "        client_box = (min(ix, x), min(iy, y), max(ix, x), max(iy, y))\n",
        "        rect_drawn = True\n",
        "\n",
        "cap.set(cv2.CAP_PROP_POS_MSEC, 1000)\n",
        "ret, frame = cap.read()\n",
        "if ret:\n",
        "    frame_copy = frame.copy()\n",
        "    cv2.namedWindow('Select Client Zone')\n",
        "    cv2.setMouseCallback('Select Client Zone', draw_rectangle)\n",
        "    print('Draw a rectangle around the client area. Press q to accept.')\n",
        "    while True:\n",
        "        cv2.imshow('Select Client Zone', frame_copy)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q') or rect_drawn:\n",
        "            break\n",
        "    cv2.destroyWindow('Select Client Zone')\n",
        "    cap.set(cv2.CAP_PROP_POS_MSEC, 0)\n",
        "else:\n",
        "    raise RuntimeError('Could not read frame for client zone selection. Check VIDEO_PATH.')\n",
        "\n",
        "def run_processing(cap, client_box, monitoring_mode=True):\n",
        "    landmark_histories = {}\n",
        "    prev_body_scores = {}\n",
        "    prev_face_scores = {}\n",
        "    client_logs = []\n",
        "    current_client_tid = None\n",
        "    client_start_time = None\n",
        "    client_scores = []\n",
        "    last_score_time = 0.0\n",
        "    agg_history = []\n",
        "    insight_text = ''\n",
        "    insight_start = 0.0\n",
        "    INSIGHT_DURATION = 10.0\n",
        "\n",
        "    cv2.namedWindow('YOLO + Pose + Face (Satisfaction)', cv2.WINDOW_NORMAL)\n",
        "    esc_pressed = False\n",
        "    last_body_frame = None\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        fh, fw = frame.shape[:2]\n",
        "        results = model.track(frame, persist=True, classes=[0], verbose=False)\n",
        "        annotated = frame.copy()\n",
        "        now = time.monotonic()\n",
        "\n",
        "        active_tracks_in_box = []\n",
        "        for box in results[0].boxes:\n",
        "            if box.id is None: continue\n",
        "            tid = int(box.id.item())\n",
        "            x1,y1,x2,y2 = map(int, box.xyxy[0].tolist())\n",
        "            overlap = box_overlap((x1,y1,x2,y2), client_box)\n",
        "            if overlap >= 0.2:\n",
        "                active_tracks_in_box.append((tid, overlap, (x1,y1,x2,y2)))\n",
        "\n",
        "        front_tid = None\n",
        "        if active_tracks_in_box:\n",
        "            high_overlap_candidates = [t for t in active_tracks_in_box if t[1] >= 0.90]\n",
        "            current_overlap = next((ov for tid,ov,_ in active_tracks_in_box if tid == current_client_tid), 0.0)\n",
        "            if current_overlap > 0:\n",
        "                front_tid = current_client_tid\n",
        "            elif high_overlap_candidates:\n",
        "                high_overlap_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "                front_tid = high_overlap_candidates[0][0]\n",
        "\n",
        "        if front_tid is not None:\n",
        "            if current_client_tid is None or current_client_tid != front_tid:\n",
        "                if current_client_tid is not None:\n",
        "                    total_time = now - client_start_time\n",
        "                    if client_scores:\n",
        "                        posture_scores = [s[0] for s in client_scores]\n",
        "                        face_scores    = [s[1] for s in client_scores]\n",
        "                        agg_scores     = [s[2] for s in client_scores]\n",
        "                        mean_posture   = np.mean(posture_scores)\n",
        "                        mean_face      = np.mean(face_scores)\n",
        "                        mean_total     = np.mean(agg_scores)\n",
        "                    else:\n",
        "                        mean_posture = mean_face = mean_total = 0\n",
        "                    print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left.\")\n",
        "                    print(f\"Time spent: {total_time:.1f}s\")\n",
        "                    print(f\"Scores list: {client_scores}\")\n",
        "                    client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
        "                    threading.Thread(target=generate_insight_thread, args=(total_time, mean_posture, mean_face, mean_total, client_scores)).start()\n",
        "\n",
        "                current_client_tid = front_tid\n",
        "                client_start_time  = now\n",
        "                client_scores      = []\n",
        "                last_score_time    = now\n",
        "                agg_history = []\n",
        "                insight_text = ''\n",
        "                print(f\"[{time.strftime('%H:%M:%S')}] New client ID {front_tid} entered.\")\n",
        "\n",
        "            tid = current_client_tid\n",
        "            if tid not in landmark_histories:\n",
        "                landmark_histories[tid] = collections.deque(maxlen=HISTORY_LEN)\n",
        "                prev_body_scores[tid] = None\n",
        "                prev_face_scores[tid] = None\n",
        "\n",
        "            person_bbox = next(b for t,_,b in active_tracks_in_box if t == tid)\n",
        "            x1, y1, x2, y2 = person_bbox\n",
        "\n",
        "            has_pose = False\n",
        "            pts = None\n",
        "            if results[0].keypoints is not None and len(results[0].keypoints) > 0:\n",
        "                for i, kp in enumerate(results[0].keypoints):\n",
        "                    if results[0].boxes[i].id is not None and int(results[0].boxes[i].id.item()) == tid:\n",
        "                        keypoints = kp.data.cpu().numpy()\n",
        "                        pts = keypoints[0, :, :2]\n",
        "                        confidences = keypoints[0, :, 2]\n",
        "                        low_conf_mask = confidences < 0.5\n",
        "                        pts[low_conf_mask] = [0, 0]\n",
        "                        pts[:, 0] /= fw; pts[:, 1] /= fh\n",
        "                        has_pose = True\n",
        "                        for idx1, idx2 in COCO_CONNECTIONS:\n",
        "                            if np.all(pts[idx1] != 0) and np.all(pts[idx2] != 0):\n",
        "                                pt1 = (int(pts[idx1][0] * fw), int(pts[idx1][1] * fh))\n",
        "                                pt2 = (int(pts[idx2][0] * fw), int(pts[idx2][1] * fh))\n",
        "                                cv2.line(annotated, pt1, pt2, (255, 0, 0), 2)\n",
        "                        break\n",
        "\n",
        "            face_box, face_crop = get_face_for_person(person_bbox, now, frame)\n",
        "            if face_box:\n",
        "                fx1, fy1, fx2, fy2 = face_box\n",
        "                cv2.rectangle(annotated, (fx1, fy1), (fx2, fy2), (0, 255, 255), 2)\n",
        "\n",
        "            cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            cv2.putText(annotated, f'ID:{tid}', (x1, y1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
        "\n",
        "            if has_pose and pts is not None and pts.shape[0] >= 17:\n",
        "                landmark_histories[tid].append(pts)\n",
        "\n",
        "            if now - last_score_time >= SCORE_INTERVAL:\n",
        "                last_score_time = now\n",
        "                body_sc = 0\n",
        "                if tid in landmark_histories and len(landmark_histories[tid]) >= 3:\n",
        "                    _, body_sc, _ = compute_satisfaction_score(landmark_histories[tid], fw, fh, prev_body_scores.get(tid))\n",
        "                    prev_body_scores[tid] = body_sc\n",
        "                face_sc = 0\n",
        "                if face_crop is not None:\n",
        "                    _, face_sc, _ = compute_face_expression_score(face_crop, prev_face_scores.get(tid))\n",
        "                    prev_face_scores[tid] = face_sc\n",
        "                scores = [s for s in (body_sc, face_sc) if s > 0]\n",
        "                agg_sc = sum(scores)/len(scores) if scores else 0\n",
        "                client_scores.append((body_sc, face_sc, agg_sc))\n",
        "                agg_history.append(agg_sc)\n",
        "\n",
        "        else:\n",
        "            if current_client_tid is not None:\n",
        "                total_time = now - client_start_time\n",
        "                if client_scores:\n",
        "                    posture_scores = [s[0] for s in client_scores]\n",
        "                    face_scores    = [s[1] for s in client_scores]\n",
        "                    agg_scores     = [s[2] for s in client_scores]\n",
        "                    mean_posture   = np.mean(posture_scores)\n",
        "                    mean_face      = np.mean(face_scores)\n",
        "                    mean_total     = np.mean(agg_scores)\n",
        "                else:\n",
        "                    mean_posture = mean_face = mean_total = 0\n",
        "                print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left.\")\n",
        "                print(f\"Time spent: {total_time:.1f}s\")\n",
        "                print(f\"Scores list: {client_scores}\")\n",
        "                client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
        "                threading.Thread(target=generate_insight_thread, args=(total_time, mean_posture, mean_face, mean_total, client_scores)).start()\n",
        "                current_client_tid = None\n",
        "\n",
        "        if client_box:\n",
        "            cv2.rectangle(annotated, (client_box[0], client_box[1]), (client_box[2], client_box[3]), (255,0,0), 2)\n",
        "\n",
        "        # display small info\n",
        "        cv2.putText(annotated, f'face : 0', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "        cv2.putText(annotated, f'posture : 0', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "        cv2.putText(annotated, f'total : 0', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "        cv2.imshow('YOLO + Pose + Face (Satisfaction)', annotated)\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == 27:\n",
        "            esc_pressed = True\n",
        "            break\n",
        "\n",
        "    if current_client_tid is not None:\n",
        "        total_time = time.monotonic() - client_start_time\n",
        "        if client_scores:\n",
        "            posture_scores = [s[0] for s in client_scores]\n",
        "            face_scores    = [s[1] for s in client_scores]\n",
        "            agg_scores     = [s[2] for s in client_scores]\n",
        "            mean_posture   = np.mean(posture_scores)\n",
        "            mean_face      = np.mean(face_scores)\n",
        "            mean_total     = np.mean(agg_scores)\n",
        "        else:\n",
        "            mean_posture = mean_face = mean_total = 0\n",
        "        print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left (video end).\")\n",
        "        client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print('All client logs:')\n",
        "    for log in client_logs:\n",
        "        print(log)\n",
        "    return client_logs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main pipeline (callable)\n",
        "\n",
        "The processing loop is wrapped in `run_processing(...)`. Run it to start; interrupt the kernel to stop. The function returns `client_logs` at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Insight generation via OpenRouter/OpenAI\n",
        "\n",
        "This cell uses the `openai` client wrapper you used previously (OpenRouter). You need `OPENROUTER_API_KEY` in your environment for it to work. It gracefully falls back on error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "client = OpenAI(base_url='https://openrouter.ai/api/v1', api_key=os.getenv('OPENROUTER_API_KEY'))\n",
        "\n",
        "def generate_insight(total_time, mean_posture, mean_face, mean_total, client_scores):\n",
        "    prompt = f\"Customer stayed for {total_time:.1f} seconds.\\nMean posture score: {mean_posture:.1f}/100\\nMean face score: {mean_face:.1f}/100\\nTotal mean satisfaction: {mean_total:.1f}/100\\nScores over time (posture, face, total): {client_scores}\\n\\nGive a short, professional insight (1-2 sentences) about possible customer mood or service issue. Focus on trends in posture (e.g. crossed arms = bored/impatient) and face expressions.\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model='meta-llama/llama-3-8b-instruct',\n",
        "            messages=[{'role':'system','content':'You are a customer service analyst. Be concise, professional, and insightful.'},\n",
        "                      {'role':'user','content':prompt}],\n",
        "            max_tokens=100\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print('Insight generation failed (no action taken):', e)\n",
        "        return ''\n",
        "\n",
        "def generate_insight_thread(total_time, mean_posture, mean_face, mean_total, client_scores):\n",
        "    insight = generate_insight(total_time, mean_posture, mean_face, mean_total, client_scores)\n",
        "    print('Insight:', insight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the pipeline\n",
        "\n",
        "After running the previous cells (including the client-zone selection), run the cell below to start processing. Interrupt the kernel to stop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logs = run_processing(cap, client_box)\n",
        "\n",
        "# You can save logs if desired:\n",
        "# import json\n",
        "# with open('client_logs.json','w') as f:\n",
        "#     json.dump(logs, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes & tips\n",
        "- For faster local testing swap `yolov8x-pose.pt` to `yolov8n-pose.pt` or similar smaller model.\n",
        "- If you run on a GPU, ensure `torch` has CUDA available and the models will use it automatically.\n",
        "- The emotion checkpoint format varies; if loading fails, consider converting to a proper state_dict matching `efficientnet_b0` head.\n",
        "- If you want frames inline in the notebook instead of OpenCV windows, I can modify the notebook to display periodic frames using `matplotlib`.\n",
        "\n",
        "---\n",
        "\n",
        "Done — the notebook file is saved and ready to download.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
