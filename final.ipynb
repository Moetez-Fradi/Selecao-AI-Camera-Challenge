{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796217be",
   "metadata": {},
   "source": [
    "# Satisfaction Monitoring Pipeline\n",
    "\n",
    "This notebook contains your full pipeline converted from the `.py` script. **Run all cells**. For real-time OpenCV windows in a Jupyter environment, enable the Qt event loop before running the main loop with:\n",
    "\n",
    "```python\n",
    "%gui qt\n",
    "```\n",
    "\n",
    "If you run on a remote environment (Colab / headless server) consider using Streamlit or saving output frames instead of relying on GUI windows.\n",
    "\n",
    "Cells are split for convenience: installation, imports & model init, helper functions, face helper, and main loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb93c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (8.3.225)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: tqdm in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: lap in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (0.5.12)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (3.10.7)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (12.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (1.16.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: psutil in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (7.1.3)\n",
      "Requirement already satisfied: polars in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (1.35.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.18 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from ultralytics) (2.0.18)\n",
      "Requirement already satisfied: absl-py in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.1 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from polars->ultralytics) (1.35.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moetez\\desktop\\cv projects\\ai vision hackathon\\.venv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics mediapipe tqdm scikit-learn opencv-python-headless opencv-python lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d4e0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "# initialize video path to be edited as needed. set it to 0 for integrated pc cam and to 1 for webcam\n",
    "cap = cv2.VideoCapture(\"./testcases/internet.mp4\")\n",
    "\n",
    "model = YOLO(\"yolov8x-pose.pt\")\n",
    "\n",
    "mp_face = mp.solutions.face_mesh\n",
    "face_mesh = mp_face.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=4,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.3,\n",
    "    min_tracking_confidence=0.3\n",
    ")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3c5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "TINY_FACE_URL  = \"https://github.com/lindevs/yolov8-face/releases/download/v1.0.0/yolov8n-face-lindevs.pt\"\n",
    "TINY_FACE_PATH = \"yolov8n-face-lindevs.pt\"\n",
    "\n",
    "if not os.path.exists(TINY_FACE_PATH):\n",
    "    print(\"Downloading tiny face model â€¦\")\n",
    "    def _download(url, dst):\n",
    "        with tqdm(unit='B', unit_scale=True, desc=os.path.basename(dst)) as t:\n",
    "            def _reporthook(b, bs, ts):\n",
    "                if ts != -1: t.total = ts\n",
    "                t.update(bs)\n",
    "            urllib.request.urlretrieve(url, dst, reporthook=_reporthook)\n",
    "    _download(TINY_FACE_URL, TINY_FACE_PATH)\n",
    "    print(\"Done!\")\n",
    "\n",
    "face_model = YOLO(TINY_FACE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8881ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_LEN               = 20\n",
    "TORSO_ACTIVITY_THRESHOLD  = 0.0025\n",
    "ARM_ACTIVITY_THRESHOLD    = 0.005\n",
    "MIN_SHOULDER_WIDTH_FRAC   = 0.10\n",
    "STATIONARY_SECONDS        = 4.0\n",
    "\n",
    "CLIENT_BOX_SCALE_W = 2.5\n",
    "CLIENT_BOX_SCALE_H = 1\n",
    "SCORE_INTERVAL   = 1.0\n",
    "CENTROID_DISP_THRESH = 0.05\n",
    "LEAVING_THRESHOLD = 0.5\n",
    "ENTERING_THRESHOLD = 0.90\n",
    "\n",
    "COCO_CONNECTIONS = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "    (5, 6), (5, 11), (6, 12), (11, 13), (13, 15), (12, 14), (14, 16), (11, 12)\n",
    "]\n",
    "\n",
    "def _angle_between_deg(v1, v2):\n",
    "    n1 = np.linalg.norm(v1); n2 = np.linalg.norm(v2)\n",
    "    if n1 == 0 or n2 == 0: return 0.0\n",
    "    c = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
    "    return math.degrees(math.acos(c))\n",
    "\n",
    "def _clamp01(x): return max(0.0, min(1.0, x))\n",
    "def _map_to_01(v, a, b):\n",
    "    if b == a: return 0.0\n",
    "    return _clamp01((v - a) / (b - a))\n",
    "\n",
    "def compute_satisfaction_score(history, fw, fh, prev_score=None, alpha=0.6):\n",
    "    if len(history) < 3: return None, 0, {}\n",
    "    pts_now  = history[-1]\n",
    "    pts_prev = history[-2]\n",
    "    pts_old  = history[-3]\n",
    "    if pts_now.shape[0] < 17: return None, 0, {}\n",
    "\n",
    "    disp1 = np.linalg.norm(pts_now - pts_prev, axis=1)\n",
    "    disp2 = np.linalg.norm(pts_prev - pts_old, axis=1)\n",
    "    activity = float((disp1.mean() + disp2.mean()) / 2.0)\n",
    "\n",
    "    L_SH, R_SH = 5, 6\n",
    "    L_HIP, R_HIP = 11, 12\n",
    "    NOSE = 0\n",
    "    L_ELB, R_ELB = 7, 8\n",
    "    L_WRIST, R_WRIST = 9, 10\n",
    "\n",
    "    shoulders = (pts_now[L_SH] + pts_now[R_SH]) / 2.0\n",
    "    hips      = (pts_now[L_HIP] + pts_now[R_HIP]) / 2.0\n",
    "    nose      = pts_now[NOSE]\n",
    "\n",
    "    torso_vec = hips - shoulders\n",
    "    torso_len = np.linalg.norm(torso_vec) + 1e-6\n",
    "    torso_dir = torso_vec / torso_len\n",
    "    vertical  = np.array([0.0, 1.0])\n",
    "    torso_angle = min(_angle_between_deg(torso_dir, vertical), 180 - _angle_between_deg(torso_dir, vertical))\n",
    "\n",
    "    head_vec = nose - shoulders\n",
    "    head_len = np.linalg.norm(head_vec) + 1e-6\n",
    "    head_dir = head_vec / head_len\n",
    "    head_torso_angle = min(_angle_between_deg(head_dir, torso_dir), 180 - _angle_between_deg(head_dir, torso_dir))\n",
    "\n",
    "    sh_ys = abs(pts_now[L_SH][1] - pts_now[R_SH][1])\n",
    "    shoulder_sym = sh_ys * fh / (torso_len * fh + 1e-6)\n",
    "\n",
    "    shoulder_width = np.linalg.norm(pts_now[L_SH] - pts_now[R_SH]) + 1e-6\n",
    "    wrist_dist     = np.linalg.norm(pts_now[L_WRIST] - pts_now[R_WRIST])\n",
    "    arm_openness   = _clamp01((wrist_dist / shoulder_width) / 2.5)\n",
    "\n",
    "    wrist_to_nose = min(np.linalg.norm(pts_now[L_WRIST] - nose),\n",
    "                        np.linalg.norm(pts_now[R_WRIST] - nose))\n",
    "    hands_face = _map_to_01(wrist_to_nose, 0.01, 0.20)\n",
    "\n",
    "    crossed_arms_penalty = 0.0\n",
    "    if all(pts_now[i].any() for i in [L_ELB, R_ELB, L_WRIST, R_WRIST, L_SH, R_SH, L_HIP, R_HIP]):\n",
    "        if (pts_now[L_WRIST][0] > pts_now[R_SH][0] and pts_now[R_WRIST][0] < pts_now[L_SH][0]) or \\\n",
    "           (pts_now[L_WRIST][0] > pts_now[R_ELB][0] and pts_now[R_WRIST][0] < pts_now[L_ELB][0]):\n",
    "            chest_y_min = min(pts_now[L_SH][1], pts_now[R_SH][1])\n",
    "            chest_y_max = max(pts_now[L_HIP][1], pts_now[R_HIP][1])\n",
    "            if chest_y_min < pts_now[L_WRIST][1] < chest_y_max and chest_y_min < pts_now[R_WRIST][1] < chest_y_max:\n",
    "                crossed_arms_penalty = 0.25\n",
    "\n",
    "    activity_score = _map_to_01(activity, 0.0008, 0.018)\n",
    "    upright_score = 1.0 if torso_angle <= 10 else 0.0 if torso_angle >= 40 else 1.0 - ((torso_angle-10)/(40-10))\n",
    "    head_align_score = _clamp01(1.0 - (head_torso_angle/40.0))\n",
    "    shoulder_sym_score = 1.0 - _clamp01(shoulder_sym*3.0)\n",
    "    hands_open_score   = hands_face\n",
    "    arm_open_score = arm_openness\n",
    "\n",
    "    combined = (0.30*upright_score + 0.25*activity_score + 0.15*hands_open_score +\n",
    "                0.15*arm_open_score + 0.10*head_align_score + 0.05*shoulder_sym_score)\n",
    "    combined -= crossed_arms_penalty\n",
    "    combined = _clamp01(combined)\n",
    "\n",
    "    if prev_score is not None:\n",
    "        combined = alpha*combined + (1-alpha)*(prev_score/100.0)\n",
    "\n",
    "    score = int(combined*100)\n",
    "    label = \"satisfied\" if score >= 70 else \"neutral\" if score >= 45 else \"dissatisfied\"\n",
    "\n",
    "    return label, score, {}\n",
    "\n",
    "def compute_face_expression_score(face_landmarks, fw, fh, prev_score=None, alpha=0.6):\n",
    "    if face_landmarks is None: return None, 0, {}\n",
    "    pts = np.array([[p.x, p.y] for p in face_landmarks.landmark], dtype=np.float32)\n",
    "    try:\n",
    "        lm_l_mouth = pts[61]; lm_r_mouth = pts[291]\n",
    "        lm_top_lip = pts[13]; lm_bottom_lip = pts[14]\n",
    "        lm_l_eye   = pts[33]; lm_r_eye   = pts[263]\n",
    "    except Exception: return None, 0, {}\n",
    "\n",
    "    mouth_w = np.linalg.norm(lm_r_mouth - lm_l_mouth)\n",
    "    mouth_h = np.linalg.norm(lm_bottom_lip - lm_top_lip)\n",
    "    eye_dist = np.linalg.norm(lm_r_eye - lm_l_eye) + 1e-6\n",
    "\n",
    "    smile_ratio = mouth_w / eye_dist\n",
    "    mouth_open_ratio = mouth_h / eye_dist\n",
    "\n",
    "    smile_score = _map_to_01(smile_ratio, 0.35, 0.75)\n",
    "    open_score  = _map_to_01(mouth_open_ratio, 0.02, 0.18)\n",
    "\n",
    "    combined = 0.8*smile_score + 0.2*open_score\n",
    "    combined = _clamp01(combined)\n",
    "\n",
    "    if prev_score is not None:\n",
    "        combined = alpha*combined + (1-alpha)*(prev_score/100.0)\n",
    "\n",
    "    score = int(combined*100)\n",
    "    label = \"satisfied\" if score >= 70 else \"neutral\" if score >= 45 else \"dissatisfied\"\n",
    "    return label, score, {}\n",
    "\n",
    "def compute_transaction_indicators(history, fw, fh):\n",
    "    if len(history) < 3: return None, {}\n",
    "    pts_now  = history[-1]; pts_prev = history[-2]; pts_old = history[-3]\n",
    "    if pts_now.shape[0] < 17: return None, {}\n",
    "\n",
    "    disp1 = np.linalg.norm(pts_now - pts_prev, axis=1)\n",
    "    disp2 = np.linalg.norm(pts_prev - pts_old, axis=1)\n",
    "\n",
    "    torso_pts = [5,6,11,12]\n",
    "    arm_pts   = [7,8,9,10]\n",
    "\n",
    "    torso_disp = (disp1[torso_pts].mean() + disp2[torso_pts].mean())/2.0\n",
    "    arm_disp   = (disp1[arm_pts].mean()   + disp2[arm_pts].mean())/2.0\n",
    "\n",
    "    shoulders = (pts_now[5] + pts_now[6]) / 2.0\n",
    "    nose      = pts_now[0]\n",
    "    head_vec  = nose - shoulders\n",
    "    head_len  = np.linalg.norm(head_vec) + 1e-6\n",
    "    head_dir  = head_vec / head_len\n",
    "    cam_dir   = np.array([0.0, -1.0])\n",
    "    facing_angle = _angle_between_deg(head_dir, cam_dir)\n",
    "\n",
    "    shoulder_width = np.linalg.norm(pts_now[5] - pts_now[6]) * fw\n",
    "    shoulder_frac  = shoulder_width / fw\n",
    "\n",
    "    diag = {\"torso_activity\":torso_disp, \"arm_activity\":arm_disp,\n",
    "            \"facing_angle\":facing_angle, \"shoulder_frac\":shoulder_frac}\n",
    "\n",
    "    is_transaction = (torso_disp < TORSO_ACTIVITY_THRESHOLD and\n",
    "                      arm_disp   > ARM_ACTIVITY_THRESHOLD and\n",
    "                      shoulder_frac > MIN_SHOULDER_WIDTH_FRAC)\n",
    "\n",
    "    return \"potential_transaction\" if is_transaction else None, diag\n",
    "\n",
    "def box_overlap(box1, box2):\n",
    "    ix1 = max(box1[0], box2[0])\n",
    "    iy1 = max(box1[1], box2[1])\n",
    "    ix2 = min(box1[2], box2[2])\n",
    "    iy2 = min(box1[3], box2[3])\n",
    "    ia  = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n",
    "    a1  = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    return ia / a1 if a1 > 0 else 0.0\n",
    "\n",
    "def bbox_area(box):\n",
    "    x1,y1,x2,y2 = box\n",
    "    return (x2 - x1) * (y2 - y1)\n",
    "\n",
    "def compute_centroid_disp(bbox_list, fw, fh):\n",
    "    if len(bbox_list) < 3: return 1.0\n",
    "    disps = []\n",
    "    for i in range(1, len(bbox_list)):\n",
    "        x1,y1,x2,y2 = bbox_list[i]\n",
    "        px1,py1,px2,py2 = bbox_list[i-1]\n",
    "        c_i = ((x1 + x2)/2 / fw, (y1 + y2)/2 / fh)\n",
    "        c_prev = ((px1 + px2)/2 / fw, (py1 + py2)/2 / fh)\n",
    "        disps.append(np.linalg.norm(np.array(c_i) - np.array(c_prev)))\n",
    "    return np.mean(disps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c48855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face helper (0.5s throttle) and cache\n",
    "last_face_check = 0.0\n",
    "cached_face_box = None\n",
    "cached_face_landmarks = None\n",
    "\n",
    "def get_face_for_person(person_bbox, now):\n",
    "    global last_face_check, cached_face_box, cached_face_landmarks\n",
    "    if now - last_face_check < 0.5:\n",
    "        if cached_face_box and box_overlap(person_bbox, cached_face_box) > 0.4:\n",
    "            return cached_face_box, cached_face_landmarks\n",
    "    x1, y1, x2, y2 = person_bbox\n",
    "    crop = frame[y1:y2, x1:x2]\n",
    "    if crop.size == 0:\n",
    "        return None, None\n",
    "    results = face_model(crop, verbose=False, conf=0.25)\n",
    "    if results[0].boxes is None or len(results[0].boxes) == 0:\n",
    "        cached_face_box = None\n",
    "        cached_face_landmarks = None\n",
    "        return None, None\n",
    "    best = results[0].boxes[0]\n",
    "    fx1, fy1, fx2, fy2 = map(int, best.xyxy[0].tolist())\n",
    "    fx1 += x1; fy1 += y1; fx2 += x1; fy2 += y1\n",
    "    face_box = (fx1, fy1, fx2, fy2)\n",
    "    crop_face_y1 = max(0, fy1 - y1)\n",
    "    crop_face_y2 = min(crop.shape[0], fy2 - y1)\n",
    "    crop_face_x1 = max(0, fx1 - x1)\n",
    "    crop_face_x2 = min(crop.shape[1], fx2 - x1)\n",
    "    face_crop = crop[crop_face_y1:crop_face_y2, crop_face_x1:crop_face_x2]\n",
    "    if face_crop.size == 0:\n",
    "        landmarks = None\n",
    "    else:\n",
    "        rgb_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "        mp_res = face_mesh.process(rgb_crop)\n",
    "        landmarks = None\n",
    "        if mp_res.multi_face_landmarks:\n",
    "            lm = mp_res.multi_face_landmarks[0]\n",
    "            fw_face = crop_face_x2 - crop_face_x1\n",
    "            fh_face = crop_face_y2 - crop_face_y1\n",
    "            for p in lm.landmark:\n",
    "                p.x = (p.x * fw_face + (fx1)) / fw\n",
    "                p.y = (p.y * fh_face + (fy1)) / fh\n",
    "            landmarks = lm\n",
    "    cached_face_box       = face_box\n",
    "    cached_face_landmarks = landmarks\n",
    "    last_face_check       = now\n",
    "    return face_box, landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9ba5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_satisfaction_graph(frame, agg_history, fw, graph_height=200):\n",
    "    if len(agg_history) == 0:\n",
    "        return frame\n",
    "    graph_frame = np.zeros((graph_height, frame.shape[1], 3), dtype=np.uint8) + 255\n",
    "    if len(agg_history) < 2:\n",
    "        cv2.putText(graph_frame, \"No data yet\", (10, graph_height // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "    else:\n",
    "        cv2.line(graph_frame, (50, graph_height - 50), (frame.shape[1] - 50, graph_height - 50), (0, 0, 0), 2)\n",
    "        cv2.line(graph_frame, (50, graph_height - 50), (50, 50), (0, 0, 0), 2)\n",
    "        cv2.putText(graph_frame, \"0\", (30, graph_height - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "        cv2.putText(graph_frame, \"100\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "        cv2.putText(graph_frame, \"Satisfaction over time\", (frame.shape[1] // 2 - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "        max_x = frame.shape[1] - 100\n",
    "        max_y = graph_height - 100\n",
    "        points = []\n",
    "        for i, score in enumerate(agg_history):\n",
    "            x = 50 + int(i * max_x / max(1, (len(agg_history) - 1)))\n",
    "            y = (graph_height - 50) - int(score / 100 * max_y)\n",
    "            points.append((x, y))\n",
    "            cv2.circle(graph_frame, (x, y), 3, (0, 0, 255), -1)\n",
    "        for i in range(1, len(points)):\n",
    "            cv2.line(graph_frame, points[i-1], points[i], (0, 0, 255), 2)\n",
    "    frame_with_graph = np.vstack((frame, graph_frame))\n",
    "    return frame_with_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094c294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:46:53] Ignored candidate ID 5: no face detected.\n",
      "[08:46:53] Ignored candidate ID 5: no face detected.\n",
      "[08:46:54] Ignored candidate ID 5: no face detected.\n",
      "[08:46:54] Ignored candidate ID 5: no face detected.\n",
      "[08:46:54] Ignored candidate ID 2: no face detected.\n",
      "[08:46:54] Ignored candidate ID 2: no face detected.\n",
      "[08:46:54] Ignored candidate ID 2: no face detected.\n",
      "[08:46:54] Ignored candidate ID 2: no face detected.\n",
      "[08:46:54] Ignored candidate ID 5: no face detected.\n",
      "[08:46:54] Ignored candidate ID 2: no face detected.\n",
      "[08:46:55] Ignored candidate ID 2: no face detected.\n",
      "[08:46:55] Ignored candidate ID 2: no face detected.\n",
      "[08:46:55] Ignored candidate ID 2: no face detected.\n",
      "[08:46:55] Ignored candidate ID 5: no face detected.\n",
      "[08:46:55] Ignored candidate ID 5: no face detected.\n",
      "[08:46:55] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:56] Ignored candidate ID 5: no face detected.\n",
      "[08:46:57] Ignored candidate ID 5: no face detected.\n",
      "[08:46:57] Ignored candidate ID 5: no face detected.\n",
      "[08:46:57] Ignored candidate ID 5: no face detected.\n",
      "[08:46:57] Ignored candidate ID 5: no face detected.\n",
      "[08:46:58] Ignored candidate ID 13: no face detected.\n",
      "[08:46:58] Ignored candidate ID 5: no face detected.\n",
      "[08:46:58] Ignored candidate ID 5: no face detected.\n",
      "[08:46:59] Guichet found at [464 278]. Switching to monitoring. Client box: (354, 212, 573, 343)\n",
      "[08:46:59] New client ID 18 entered.\n",
      "[08:47:04] Client ID 18 left.\n",
      "Time spent: 5.1s\n",
      "Scores list: [(42, 0, 42.0), (60, 0, 60.0), (67, 0, 67.0), (70, 0, 70.0)]\n",
      "Mean posture: 59.8\n",
      "Mean face: 0.0\n",
      "Total mean score: 59.8\n",
      "[08:47:13] New client ID 70 entered.\n",
      "[08:47:17] Client ID 70 left.\n",
      "Time spent: 3.6s\n",
      "Scores list: [(44, 23, 33.5), (52, 13, 32.5), (53, 14, 33.5)]\n",
      "Mean posture: 49.7\n",
      "Mean face: 16.7\n",
      "Total mean score: 33.2\n",
      "All client logs:\n",
      "(18, 5.092999999993481, 59.75, 0.0, 59.75)\n",
      "(70, 3.625, 49.666666666666664, 16.666666666666668, 33.166666666666664)\n"
     ]
    }
   ],
   "source": [
    "# MAIN LOOP\n",
    "landmark_histories      = {}\n",
    "torso_activity_histories = {}\n",
    "bbox_histories          = {}\n",
    "stationary_since        = {}\n",
    "prev_body_scores        = {}\n",
    "prev_face_scores        = {}\n",
    "stationary_start_time = {}\n",
    "last_print = 0.0\n",
    "transaction_locations = []\n",
    "current_client_tid = None\n",
    "client_start_time  = None\n",
    "client_scores      = []\n",
    "last_score_time    = 0.0\n",
    "client_logs        = []\n",
    "guichet_loc   = None\n",
    "client_box    = None\n",
    "current_face_sc = 0\n",
    "current_body_sc = 0\n",
    "current_agg_sc = 0\n",
    "agg_history = []\n",
    "\n",
    "cv2.namedWindow(\"YOLO + Pose + Face (Satisfaction)\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    fh, fw = frame.shape[:2]\n",
    "    results  = model.track(frame, persist=True, classes=[0], verbose=False)\n",
    "    annotated = frame.copy()\n",
    "    now = time.monotonic()\n",
    "\n",
    "    if guichet_loc is None:\n",
    "        current_candidates = {}\n",
    "        for box in results[0].boxes:\n",
    "            if box.id is None: continue\n",
    "            tid = int(box.id.item())\n",
    "            x1,y1,x2,y2 = map(int, box.xyxy[0].tolist())\n",
    "            if tid not in bbox_histories:\n",
    "                bbox_histories[tid] = collections.deque(maxlen=30)\n",
    "            bbox_histories[tid].append((x1,y1,x2,y2))\n",
    "            area = bbox_area((x1,y1,x2,y2))\n",
    "            cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            if len(bbox_histories[tid]) >= 15:\n",
    "                avg_disp = compute_centroid_disp(list(bbox_histories[tid])[-15:], fw, fh)\n",
    "                if avg_disp < CENTROID_DISP_THRESH:\n",
    "                    if tid not in stationary_start_time:\n",
    "                        stationary_start_time[tid] = now\n",
    "                    elapsed = now - stationary_start_time[tid]\n",
    "                    if elapsed >= STATIONARY_SECONDS:\n",
    "                        current_candidates[tid] = (area, (cx, cy), elapsed)\n",
    "                else:\n",
    "                    if tid in stationary_start_time:\n",
    "                        del stationary_start_time[tid]\n",
    "            color = (0, 255, 0) if tid in current_candidates else (0, 0, 255)\n",
    "            cv2.rectangle(annotated, (x1,y1), (x2,y2), color, 2)\n",
    "            status = f\"ID:{tid} {area:.0f}\"\n",
    "            if tid in current_candidates:\n",
    "                status += f\" [{current_candidates[tid][2]:.1f}s]\"\n",
    "            cv2.putText(annotated, status, (x1, y1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "        if current_candidates:\n",
    "            best_tid = max(current_candidates, key=lambda tid: current_candidates[tid][0])\n",
    "            _, best_centroid, _ = current_candidates[best_tid]\n",
    "            best_bbox = bbox_histories[best_tid][-1]\n",
    "            face_box, _ = get_face_for_person(best_bbox, now)\n",
    "            if face_box is not None:\n",
    "                transaction_locations.append(best_centroid)\n",
    "            else:\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Ignored candidate ID {best_tid}: no face detected.\")\n",
    "        if len(transaction_locations) > 10:\n",
    "            from sklearn.cluster import DBSCAN\n",
    "            locs = np.array(transaction_locations)\n",
    "            cl = DBSCAN(eps=50, min_samples=3).fit(locs)\n",
    "            lbls = cl.labels_\n",
    "            if len(np.unique(lbls[lbls >= 0])) > 0:\n",
    "                main = np.argmax(np.bincount(lbls[lbls >= 0]))\n",
    "                guichet_loc = np.mean(locs[lbls == main], axis=0).astype(int)\n",
    "                avg_w = np.mean([bbox_area(b) for hist in bbox_histories.values() for b in hist]) ** 0.5\n",
    "                avg_h = avg_w * 1.5\n",
    "                cx, cy = guichet_loc\n",
    "                client_box = (int(cx - avg_w/2 * CLIENT_BOX_SCALE_W),\n",
    "                              int(cy - avg_h/2 * CLIENT_BOX_SCALE_H),\n",
    "                              int(cx + avg_w/2 * CLIENT_BOX_SCALE_W),\n",
    "                              int(cy + avg_h/2 * CLIENT_BOX_SCALE_H))\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Guichet found at {guichet_loc}. Switching to monitoring. Client box: {client_box}\")\n",
    "    else:\n",
    "        active_tracks_in_box = []\n",
    "        for box in results[0].boxes:\n",
    "            if box.id is None: continue\n",
    "            tid = int(box.id.item())\n",
    "            x1,y1,x2,y2 = map(int, box.xyxy[0].tolist())\n",
    "            overlap = box_overlap((x1,y1,x2,y2), client_box)\n",
    "            if overlap >= LEAVING_THRESHOLD:\n",
    "                active_tracks_in_box.append((tid, overlap, (x1,y1,x2,y2)))\n",
    "        front_tid = None\n",
    "        if active_tracks_in_box:\n",
    "            high_overlap_candidates = [t for t in active_tracks_in_box if t[1] >= ENTERING_THRESHOLD]\n",
    "            current_overlap = next((ov for tid,ov,_ in active_tracks_in_box if tid == current_client_tid), 0.0)\n",
    "            if current_overlap > 0:\n",
    "                front_tid = current_client_tid\n",
    "            elif high_overlap_candidates:\n",
    "                high_overlap_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "                front_tid = high_overlap_candidates[0][0]\n",
    "        if front_tid is not None:\n",
    "            if current_client_tid is None or current_client_tid != front_tid:\n",
    "                if current_client_tid is not None:\n",
    "                    total_time = now - client_start_time\n",
    "                    if client_scores:\n",
    "                        posture_scores = [s[0] for s in client_scores]\n",
    "                        face_scores    = [s[1] for s in client_scores]\n",
    "                        agg_scores     = [s[2] for s in client_scores]\n",
    "                        mean_posture   = np.mean(posture_scores)\n",
    "                        mean_face      = np.mean(face_scores)\n",
    "                        mean_total     = np.mean(agg_scores)\n",
    "                    else:\n",
    "                        mean_posture = mean_face = mean_total = 0\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left.\")\n",
    "                    print(f\"Time spent: {total_time:.1f}s\")\n",
    "                    print(f\"Scores list: {client_scores}\")\n",
    "                    print(f\"Mean posture: {mean_posture:.1f}\")\n",
    "                    print(f\"Mean face: {mean_face:.1f}\")\n",
    "                    print(f\"Total mean score: {mean_total:.1f}\")\n",
    "                    client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
    "                current_client_tid = front_tid\n",
    "                client_start_time  = now\n",
    "                client_scores      = []\n",
    "                last_score_time    = now\n",
    "                agg_history = []\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] New client ID {front_tid} entered.\")\n",
    "            tid = current_client_tid\n",
    "            if tid not in landmark_histories:\n",
    "                landmark_histories[tid]       = collections.deque(maxlen=HISTORY_LEN)\n",
    "                torso_activity_histories[tid] = collections.deque(maxlen=5)\n",
    "                prev_body_scores[tid]         = None\n",
    "                prev_face_scores[tid]         = None\n",
    "            person_bbox = next(b for t,o,b in active_tracks_in_box if t == tid)\n",
    "            x1, y1, x2, y2 = person_bbox\n",
    "            has_pose = False\n",
    "            pts = None\n",
    "            if results[0].keypoints is not None and len(results[0].keypoints) > 0:\n",
    "                for i, kp in enumerate(results[0].keypoints):\n",
    "                    if results[0].boxes[i].id is not None and int(results[0].boxes[i].id.item()) == tid:\n",
    "                        keypoints = kp.data.cpu().numpy()\n",
    "                        pts = keypoints[0, :, :2]\n",
    "                        confidences = keypoints[0, :, 2]\n",
    "                        low_conf_mask = confidences < 0.5\n",
    "                        pts[low_conf_mask] = [0, 0]\n",
    "                        pts[:, 0] /= fw\n",
    "                        pts[:, 1] /= fh\n",
    "                        has_pose = True\n",
    "                        for idx1, idx2 in COCO_CONNECTIONS:\n",
    "                            if np.all(pts[idx1] != 0) and np.all(pts[idx2] != 0):\n",
    "                                pt1 = (int(pts[idx1][0] * fw), int(pts[idx1][1] * fh))\n",
    "                                pt2 = (int(pts[idx2][0] * fw), int(pts[idx2][1] * fh))\n",
    "                                cv2.line(annotated, pt1, pt2, (255, 0, 0), 2)\n",
    "                        break\n",
    "            face_box, face_landmarks = get_face_for_person(person_bbox, now)\n",
    "            if face_box:\n",
    "                fx1, fy1, fx2, fy2 = face_box\n",
    "                cv2.rectangle(annotated, (fx1, fy1), (fx2, fy2), (0, 255, 255), 2)\n",
    "            if face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated,\n",
    "                    face_landmarks,\n",
    "                    mp_face.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,100,0), thickness=0, circle_radius=0),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1)\n",
    "                )\n",
    "            cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "            cv2.putText(annotated, f\"ID:{tid}\", (x1, y1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "            if has_pose and pts is not None and pts.shape[0] >= 17:\n",
    "                landmark_histories[tid].append(pts)\n",
    "            if now - last_score_time >= SCORE_INTERVAL:\n",
    "                last_score_time = now\n",
    "                body_sc = 0\n",
    "                if tid in landmark_histories and len(landmark_histories[tid]) >= 3:\n",
    "                    _, body_sc, _ = compute_satisfaction_score(landmark_histories[tid], fw, fh, prev_body_scores.get(tid))\n",
    "                    prev_body_scores[tid] = body_sc\n",
    "                face_sc = 0\n",
    "                if face_landmarks:\n",
    "                    _, face_sc, _ = compute_face_expression_score(face_landmarks, fw, fh, prev_face_scores.get(tid))\n",
    "                    prev_face_scores[tid] = face_sc\n",
    "                scores = [s for s in (body_sc, face_sc) if s > 0]\n",
    "                agg_sc = sum(scores)/len(scores) if scores else 0\n",
    "                client_scores.append((body_sc, face_sc, agg_sc))\n",
    "                agg_history.append(agg_sc)\n",
    "                current_face_sc = face_sc\n",
    "                current_body_sc = body_sc\n",
    "                current_agg_sc = agg_sc\n",
    "        else:\n",
    "            if current_client_tid is not None:\n",
    "                total_time = now - client_start_time\n",
    "                if client_scores:\n",
    "                    posture_scores = [s[0] for s in client_scores]\n",
    "                    face_scores    = [s[1] for s in client_scores]\n",
    "                    agg_scores     = [s[2] for s in client_scores]\n",
    "                    mean_posture   = np.mean(posture_scores)\n",
    "                    mean_face      = np.mean(face_scores)\n",
    "                    mean_total     = np.mean(agg_scores)\n",
    "                else:\n",
    "                    mean_posture = mean_face = mean_total = 0\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left.\")\n",
    "                print(f\"Time spent: {total_time:.1f}s\")\n",
    "                print(f\"Scores list: {client_scores}\")\n",
    "                print(f\"Mean posture: {mean_posture:.1f}\")\n",
    "                print(f\"Mean face: {mean_face:.1f}\")\n",
    "                print(f\"Total mean score: {mean_total:.1f}\")\n",
    "                client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
    "                current_client_tid = None\n",
    "        if client_box:\n",
    "            cv2.rectangle(annotated, (client_box[0], client_box[1]), (client_box[2], client_box[3]), (255,0,0), 2)\n",
    "    cv2.putText(annotated, f\"face : {current_face_sc}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated, f\"posture : {current_body_sc}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated, f\"total : {current_agg_sc}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    if guichet_loc is not None and len(agg_history) > 0:\n",
    "        annotated = draw_satisfaction_graph(annotated, agg_history, annotated.shape[1])\n",
    "    cv2.imshow(\"YOLO + Pose + Face (Satisfaction)\", annotated)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "if current_client_tid is not None:\n",
    "    total_time = now - client_start_time\n",
    "    if client_scores:\n",
    "        posture_scores = [s[0] for s in client_scores]\n",
    "        face_scores    = [s[1] for s in client_scores]\n",
    "        agg_scores     = [s[2] for s in client_scores]\n",
    "        mean_posture   = np.mean(posture_scores)\n",
    "        mean_face      = np.mean(face_scores)\n",
    "        mean_total     = np.mean(agg_scores)\n",
    "    else:\n",
    "        mean_posture = mean_face = mean_total = 0\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left (video end).\")\n",
    "    print(f\"Time spent: {total_time:.1f}s\")\n",
    "    print(f\"Scores list: {client_scores}\")\n",
    "    print(f\"Mean posture: {mean_posture:.1f}\")\n",
    "    print(f\"Mean face: {mean_face:.1f}\")\n",
    "    print(f\"Total mean score: {mean_total:.1f}\")\n",
    "    client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
    "\n",
    "print(\"All client logs:\")\n",
    "for log in client_logs:\n",
    "    print(log)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
