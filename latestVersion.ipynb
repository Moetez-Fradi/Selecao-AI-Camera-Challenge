{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Satisfaction Monitor (YOLO + Pose + Face)\n",
        "This notebook contains the complete script you provided, packaged as a single executable cell. \n",
        "**Notes (run locally):**\n",
        "- This notebook is intended to be run on a machine with a camera and the required packages installed (`ultralytics`, `timm`, `torch`, `opencv-python`, `python-dotenv`, `openai` or `openrouter` client, `tqdm`).  \n",
        "- Install packages in your environment before running the notebook (examples provided below). Internet required for pip installs and model downloads.\n",
        "- Set environment variables (e.g. `OPENROUTER_API_KEY`) in a `.env` file or your environment.\n",
        "- The notebook preserves the original script structure; some systems (e.g. headless servers) may require modifications (no GUI)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "Run these commands in your terminal (not necessarily inside the notebook) if needed:\n",
        "```\n",
        "pip install ultralytics timm torch torchvision opencv-python python-dotenv tqdm openai\n",
        "```\n",
        "Put your OpenRouter API key into a `.env` file or export it in the shell:\n",
        "```\n",
        "echo \"OPENROUTER_API_KEY=your_key_here\" > .env\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c9ae9ac0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2827f0c6",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'timm'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimm\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'timm'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import timm\n",
        "from openai import OpenAI\n",
        "import dotenv\n",
        "import threading\n",
        "\n",
        "dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326685fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = YOLO(\"yolov8x-pose.pt\")\n",
        "cap   = cv2.VideoCapture(1)\n",
        "\n",
        "TINY_FACE_URL  = \"https://github.com/lindevs/yolov8-face/releases/download/v1.0.0/yolov8n-face-lindevs.pt\"\n",
        "TINY_FACE_PATH = \"yolov8n-face-lindevs.pt\"\n",
        "\n",
        "if not os.path.exists(TINY_FACE_PATH):\n",
        "    print(\"Downloading tiny face model …\")\n",
        "    def _download(url, dst):\n",
        "        with tqdm(unit='B', unit_scale=True, desc=os.path.basename(dst)) as t:\n",
        "            def _reporthook(b, bs, ts):\n",
        "                if ts != -1: t.total = ts\n",
        "                t.update(bs)\n",
        "            urllib.request.urlretrieve(url, dst, reporthook=_reporthook)\n",
        "    _download(TINY_FACE_URL, TINY_FACE_PATH)\n",
        "    print(\"Done!\")\n",
        "\n",
        "face_model = YOLO(TINY_FACE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b481935",
      "metadata": {},
      "outputs": [],
      "source": [
        "EMOTION_MODEL_URL = \"https://github.com/sb-ai-lab/EmotiEffLib/raw/main/models/affectnet_emotions/enet_b0_8_best_afew.pt\"\n",
        "EMOTION_MODEL_PATH = \"enet_b0_8_best_afew.pt\"\n",
        "\n",
        "if not os.path.exists(EMOTION_MODEL_PATH):\n",
        "    print(\"Downloading emotion model …\")\n",
        "    def _download(url, dst):\n",
        "        with tqdm(unit='B', unit_scale=True, desc=os.path.basename(dst)) as t:\n",
        "            def _reporthook(b, bs, ts):\n",
        "                if ts != -1: t.total = ts\n",
        "                t.update(bs)\n",
        "            urllib.request.urlretrieve(url, dst, reporthook=_reporthook)\n",
        "    _download(EMOTION_MODEL_URL, EMOTION_MODEL_PATH)\n",
        "    print(\"Done!\")\n",
        "\n",
        "emotion_model = timm.create_model('efficientnet_b0', num_classes=8, pretrained=False)\n",
        "emotion_model = torch.load(EMOTION_MODEL_PATH, map_location='cpu', weights_only=False)\n",
        "emotion_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e971b1c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "HISTORY_LEN               = 20\n",
        "TORSO_ACTIVITY_THRESHOLD  = 0.0025\n",
        "ARM_ACTIVITY_THRESHOLD    = 0.005\n",
        "MIN_SHOULDER_WIDTH_FRAC   = 0.10\n",
        "STATIONARY_SECONDS        = 4.0\n",
        "\n",
        "CLIENT_BOX_SCALE_W = 3\n",
        "CLIENT_BOX_SCALE_H = 1\n",
        "SCORE_INTERVAL   = 1.0\n",
        "CENTROID_DISP_THRESH = 0.05\n",
        "LEAVING_THRESHOLD = 0.2\n",
        "ENTERING_THRESHOLD = 0.90\n",
        "\n",
        "COCO_CONNECTIONS = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (0, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
        "    (5, 6), (5, 11), (6, 12), (11, 13), (13, 15), (12, 14), (14, 16), (11, 12)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fb2f3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _angle_between_deg(v1, v2):\n",
        "    n1 = np.linalg.norm(v1); n2 = np.linalg.norm(v2)\n",
        "    if n1 == 0 or n2 == 0: return 0.0\n",
        "    c = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n",
        "    return math.degrees(math.acos(c))\n",
        "\n",
        "def _clamp01(x): return max(0.0, min(1.0, x))\n",
        "def _map_to_01(v, a, b):\n",
        "    if b == a: return 0.0\n",
        "    return _clamp01((v - a) / (b - a))\n",
        "\n",
        "def compute_satisfaction_score(history, fw, fh, prev_score=None, alpha=0.6):\n",
        "    if len(history) < 3: return None, 0, {}\n",
        "    pts_now  = history[-1]\n",
        "    pts_prev = history[-2]\n",
        "    pts_old  = history[-3]\n",
        "    if pts_now.shape[0] < 17: return None, 0, {}\n",
        "\n",
        "    disp1 = np.linalg.norm(pts_now - pts_prev, axis=1)\n",
        "    disp2 = np.linalg.norm(pts_prev - pts_old, axis=1)\n",
        "    activity = float((disp1.mean() + disp2.mean()) / 2.0)\n",
        "\n",
        "    L_SH, R_SH = 5, 6\n",
        "    L_HIP, R_HIP = 11, 12\n",
        "    NOSE = 0\n",
        "    L_ELB, R_ELB = 7, 8\n",
        "    L_WRIST, R_WRIST = 9, 10\n",
        "\n",
        "    shoulders = (pts_now[L_SH] + pts_now[R_SH]) / 2.0\n",
        "    hips      = (pts_now[L_HIP] + pts_now[R_HIP]) / 2.0\n",
        "    nose      = pts_now[NOSE]\n",
        "\n",
        "    torso_vec = hips - shoulders\n",
        "    torso_len = np.linalg.norm(torso_vec) + 1e-6\n",
        "    torso_dir = torso_vec / torso_len\n",
        "    vertical  = np.array([0.0, 1.0])\n",
        "    torso_angle = min(_angle_between_deg(torso_dir, vertical), 180 - _angle_between_deg(torso_dir, vertical))\n",
        "\n",
        "    head_vec = nose - shoulders\n",
        "    head_len = np.linalg.norm(head_vec) + 1e-6\n",
        "    head_dir = head_vec / head_len\n",
        "    head_torso_angle = min(_angle_between_deg(head_dir, torso_dir), 180 - _angle_between_deg(head_dir, torso_dir))\n",
        "\n",
        "    sh_ys = abs(pts_now[L_SH][1] - pts_now[R_SH][1])\n",
        "    shoulder_sym = sh_ys * fh / (torso_len * fh + 1e-6)\n",
        "\n",
        "    shoulder_width = np.linalg.norm(pts_now[L_SH] - pts_now[R_SH]) + 1e-6\n",
        "    wrist_dist     = np.linalg.norm(pts_now[L_WRIST] - pts_now[R_WRIST])\n",
        "    arm_openness   = _clamp01((wrist_dist / shoulder_width) / 2.5)\n",
        "\n",
        "    wrist_to_nose = min(np.linalg.norm(pts_now[L_WRIST] - nose),\n",
        "                        np.linalg.norm(pts_now[R_WRIST] - nose))\n",
        "    hands_face = _map_to_01(wrist_to_nose, 0.01, 0.20)\n",
        "\n",
        "    crossed_arms_penalty = 0.0\n",
        "    if all(pts_now[i].any() for i in [L_ELB, R_ELB, L_WRIST, R_WRIST, L_SH, R_SH, L_HIP, R_HIP]):\n",
        "        if (pts_now[L_WRIST][0] > pts_now[R_SH][0] and pts_now[R_WRIST][0] < pts_now[L_SH][0]) or (pts_now[L_WRIST][0] > pts_now[R_ELB][0] and pts_now[R_WRIST][0] < pts_now[L_ELB][0]):\n",
        "            chest_y_min = min(pts_now[L_SH][1], pts_now[R_SH][1])\n",
        "            chest_y_max = max(pts_now[L_HIP][1], pts_now[R_HIP][1])\n",
        "            if chest_y_min < pts_now[L_WRIST][1] < chest_y_max and chest_y_min < pts_now[R_WRIST][1] < chest_y_max:\n",
        "                left_arm_vec = pts_now[L_WRIST] - pts_now[L_ELB]\n",
        "                right_arm_vec = pts_now[R_WRIST] - pts_now[R_ELB]\n",
        "                cross_prod = left_arm_vec[0] * right_arm_vec[1] - left_arm_vec[1] * right_arm_vec[0]\n",
        "                if abs(cross_prod) > 0.01:\n",
        "                    crossed_arms_penalty = 0.50\n",
        "\n",
        "    activity_score = _map_to_01(activity, 0.0008, 0.018)\n",
        "    upright_score = 1.0 if torso_angle <= 10 else 0.0 if torso_angle >= 40 else 1.0 - ((torso_angle-10)/(40-10))\n",
        "    head_align_score = _clamp01(1.0 - (head_torso_angle/40.0))\n",
        "    shoulder_sym_score = 1.0 - _clamp01(shoulder_sym*3.0)\n",
        "    hands_open_score   = hands_face\n",
        "    arm_open_score = arm_openness\n",
        "\n",
        "    combined = (0.40*upright_score + 0.20*head_align_score + 0.10*activity_score + \n",
        "                0.10*hands_open_score + 0.10*arm_open_score + 0.10*shoulder_sym_score)\n",
        "    combined -= crossed_arms_penalty\n",
        "\n",
        "    harsh_penalty = 0.0\n",
        "    if activity > 0.018:\n",
        "        harsh_penalty = 0.30\n",
        "    combined -= harsh_penalty\n",
        "\n",
        "    close_arms_penalty = 0.0\n",
        "    if arm_openness < 0.3:\n",
        "        close_arms_penalty = 0.20\n",
        "    combined -= close_arms_penalty\n",
        "\n",
        "    relaxed_boost = 0.0\n",
        "    if arm_openness > 0.7 and upright_score > 0.8 and activity < 0.005:\n",
        "        relaxed_boost = 0.20\n",
        "    combined += relaxed_boost\n",
        "\n",
        "    combined = _clamp01(combined)\n",
        "\n",
        "    if prev_score is not None:\n",
        "        combined = alpha*combined + (1-alpha)*(prev_score/100.0)\n",
        "\n",
        "    score = int(combined*100)\n",
        "\n",
        "    if harsh_penalty > 0:\n",
        "        label = \"angry\"\n",
        "    elif crossed_arms_penalty > 0:\n",
        "        label = \"bored\"\n",
        "    elif close_arms_penalty > 0:\n",
        "        label = \"uncomfortable\"\n",
        "    elif relaxed_boost > 0:\n",
        "        label = \"comfortable\"\n",
        "    else:\n",
        "        label = \"satisfied\" if score >= 70 else \"neutral\" if score >= 45 else \"dissatisfied\"\n",
        "\n",
        "    return label, score, {}\n",
        "\n",
        "def compute_face_expression_score(face_crop, prev_score=None, alpha=0.6):\n",
        "    if face_crop is None or face_crop.size == 0: return None, 0, {}\n",
        "    resized = cv2.resize(face_crop, (224, 224))\n",
        "    normalized = resized / 255.0\n",
        "    tensor = torch.tensor(normalized, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    tensor = (tensor - mean) / std\n",
        "    with torch.no_grad():\n",
        "        output = emotion_model(tensor)\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "    happy_prob = probs[0,4].item()\n",
        "    neutral_prob = probs[0,5].item()\n",
        "    surprise_prob = probs[0,7].item()\n",
        "    score = happy_prob * 100 + neutral_prob * 50 + surprise_prob * 30\n",
        "    score = min(score, 100.0)\n",
        "    if prev_score is not None:\n",
        "        score = alpha * score + (1 - alpha) * prev_score\n",
        "    label = \"satisfied\" if score >= 70 else \"neutral\" if score >= 45 else \"dissatisfied\"\n",
        "    return label, int(score), {}\n",
        "\n",
        "def compute_transaction_indicators(history, fw, fh):\n",
        "    if len(history) < 3: return None, {}\n",
        "    pts_now  = history[-1]; pts_prev = history[-2]; pts_old = history[-3]\n",
        "    if pts_now.shape[0] < 17: return None, {}\n",
        "\n",
        "    disp1 = np.linalg.norm(pts_now - pts_prev, axis=1)\n",
        "    disp2 = np.linalg.norm(pts_prev - pts_old, axis=1)\n",
        "\n",
        "    torso_pts = [5,6,11,12]\n",
        "    arm_pts   = [7,8,9,10]\n",
        "\n",
        "    torso_disp = (disp1[torso_pts].mean() + disp2[torso_pts].mean())/2.0\n",
        "    arm_disp   = (disp1[arm_pts].mean()   + disp2[arm_pts].mean())/2.0\n",
        "\n",
        "    shoulders = (pts_now[5] + pts_now[6]) / 2.0\n",
        "    nose      = pts_now[0]\n",
        "    head_vec  = nose - shoulders\n",
        "    head_len  = np.linalg.norm(head_vec) + 1e-6\n",
        "    head_dir  = head_vec / head_len\n",
        "    cam_dir   = np.array([0.0, -1.0])\n",
        "    facing_angle = _angle_between_deg(head_dir, cam_dir)\n",
        "\n",
        "    shoulder_width = np.linalg.norm(pts_now[5] - pts_now[6]) * fw\n",
        "    shoulder_frac  = shoulder_width / fw\n",
        "\n",
        "    diag = {\"torso_activity\":torso_disp, \"arm_activity\":arm_disp,\n",
        "            \"facing_angle\":facing_angle, \"shoulder_frac\":shoulder_frac}\n",
        "\n",
        "    is_transaction = (torso_disp < TORSO_ACTIVITY_THRESHOLD and\n",
        "                      arm_disp   > ARM_ACTIVITY_THRESHOLD and\n",
        "                      shoulder_frac > MIN_SHOULDER_WIDTH_FRAC)\n",
        "\n",
        "    return \"potential_transaction\" if is_transaction else None, diag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d528b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "landmark_histories      = {}\n",
        "torso_activity_histories = {}\n",
        "bbox_histories          = {}\n",
        "stationary_since        = {}\n",
        "prev_body_scores        = {}\n",
        "prev_face_scores        = {}\n",
        "stationary_start_time = {}\n",
        "last_print = 0.0\n",
        "transaction_locations = []\n",
        "current_client_tid = None\n",
        "client_start_time  = None\n",
        "client_scores      = []\n",
        "last_score_time    = 0.0\n",
        "client_logs        = []\n",
        "client_box    = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6552197f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def box_overlap(box1, box2):\n",
        "    ix1 = max(box1[0], box2[0])\n",
        "    iy1 = max(box1[1], box2[1])\n",
        "    ix2 = min(box1[2], box2[2])\n",
        "    iy2 = min(box1[3], box2[3])\n",
        "    ia  = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n",
        "    a1  = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    return ia / a1 if a1 > 0 else 0.0\n",
        "\n",
        "def bbox_area(box):\n",
        "    x1,y1,x2,y2 = box\n",
        "    return (x2 - x1) * (y2 - y1)\n",
        "\n",
        "def compute_centroid_disp(bbox_list, fw, fh):\n",
        "    if len(bbox_list) < 3: return 1.0\n",
        "    disps = []\n",
        "    for i in range(1, len(bbox_list)):\n",
        "        x1,y1,x2,y2 = bbox_list[i]\n",
        "        px1,py1,px2,py2 = bbox_list[i-1]\n",
        "        c_i = ((x1 + x2)/2 / fw, (y1 + y2)/2 / fh)\n",
        "        c_prev = ((px1 + px2)/2 / fw, (py1 + py2)/2 / fh)\n",
        "        disps.append(np.linalg.norm(np.array(c_i) - np.array(c_prev)))\n",
        "    return np.mean(disps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b293f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "last_face_check = 0.0\n",
        "cached_face_box = None\n",
        "cached_face_crop = None\n",
        "\n",
        "def get_face_for_person(person_bbox, now):\n",
        "    global last_face_check, cached_face_box, cached_face_crop\n",
        "    if now - last_face_check < 0.5:\n",
        "        if cached_face_box and box_overlap(person_bbox, cached_face_box) > 0.4:\n",
        "            return cached_face_box, cached_face_crop\n",
        "    x1, y1, x2, y2 = person_bbox\n",
        "    crop = frame[y1:y2, x1:x2]\n",
        "    if crop.size == 0:\n",
        "        return None, None\n",
        "\n",
        "    results = face_model(crop, verbose=False, conf=0.25)\n",
        "    if results[0].boxes is None or len(results[0].boxes) == 0:\n",
        "        cached_face_box = None\n",
        "        cached_face_crop = None\n",
        "        return None, None\n",
        "\n",
        "    best = results[0].boxes[0]\n",
        "    fx1, fy1, fx2, fy2 = map(int, best.xyxy[0].tolist())\n",
        "    fx1 += x1; fy1 += y1; fx2 += x1; fy2 += y1\n",
        "    face_box = (fx1, fy1, fx2, fy2)\n",
        "    crop_face_y1 = max(0, fy1 - y1)\n",
        "    crop_face_y2 = min(crop.shape[0], fy2 - y1)\n",
        "    crop_face_x1 = max(0, fx1 - x1)\n",
        "    crop_face_x2 = min(crop.shape[1], fx2 - x1)\n",
        "    face_crop = crop[crop_face_y1:crop_face_y2, crop_face_x1:crop_face_x2]\n",
        "    if face_crop.size == 0:\n",
        "        face_crop = None\n",
        "\n",
        "    cached_face_box       = face_box\n",
        "    cached_face_crop = face_crop\n",
        "    last_face_check       = now\n",
        "    return face_box, face_crop\n",
        "\n",
        "def draw_satisfaction_graph(frame, agg_history, fw, graph_height=200):\n",
        "    if len(agg_history) == 0:\n",
        "        return frame\n",
        "    graph_frame = np.zeros((graph_height, frame.shape[1], 3), dtype=np.uint8) + 255\n",
        "    if len(agg_history) < 2:\n",
        "        cv2.putText(graph_frame, \"No data yet\", (10, graph_height // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
        "    else:\n",
        "        cv2.line(graph_frame, (50, graph_height - 50), (frame.shape[1] - 50, graph_height - 50), (0, 0, 0), 2)\n",
        "        cv2.line(graph_frame, (50, graph_height - 50), (50, 50), (0, 0, 0), 2)\n",
        "        cv2.putText(graph_frame, \"0\", (30, graph_height - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "        cv2.putText(graph_frame, \"100\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "        cv2.putText(graph_frame, \"Satisfaction over time\", (frame.shape[1] // 2 - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
        "\n",
        "\n",
        "    frame_with_graph = np.vstack((frame, graph_frame))\n",
        "    return frame_with_graph\n",
        "\n",
        "drawing = False\n",
        "ix, iy = -1, -1\n",
        "rect_drawn = False\n",
        "\n",
        "def draw_rectangle(event, x, y, flags, param):\n",
        "    global ix, iy, drawing, frame_copy, rect_drawn, client_box\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:\n",
        "        drawing = True\n",
        "        ix, iy = x, y\n",
        "    elif event == cv2.EVENT_MOUSEMOVE:\n",
        "        if drawing:\n",
        "            img = frame_copy.copy()\n",
        "            cv2.rectangle(img, (ix, iy), (x, y), (0, 255, 0), 2)\n",
        "            cv2.imshow('Select Client Zone', img)\n",
        "    elif event == cv2.EVENT_LBUTTONUP:\n",
        "        drawing = False\n",
        "        cv2.rectangle(frame_copy, (ix, iy), (x, y), (0, 255, 0), 2)\n",
        "        cv2.imshow('Select Client Zone', frame_copy)\n",
        "        client_box = (min(ix, x), min(iy, y), max(ix, x), max(iy, y))\n",
        "        rect_drawn = True\n",
        "\n",
        "cap.set(cv2.CAP_PROP_POS_MSEC, 1000)\n",
        "ret, frame = cap.read()\n",
        "if ret:\n",
        "    frame_copy = frame.copy()\n",
        "    cv2.namedWindow('Select Client Zone')\n",
        "    cv2.setMouseCallback('Select Client Zone', draw_rectangle)\n",
        "    while True:\n",
        "        cv2.imshow('Select Client Zone', frame_copy)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q') or rect_drawn:\n",
        "            break\n",
        "    cv2.destroyWindow('Select Client Zone')\n",
        "cap.set(cv2.CAP_PROP_POS_MSEC, 0)\n",
        "\n",
        "monitoring_mode = True\n",
        "\n",
        "current_face_sc = 0\n",
        "current_body_sc = 0\n",
        "current_agg_sc = 0\n",
        "\n",
        "agg_history = []\n",
        "\n",
        "insight_text   = \"\"\n",
        "insight_start  = 0.0\n",
        "INSIGHT_DURATION = 10.0\n",
        "\n",
        "cv2.namedWindow(\"Selecao Project\", cv2.WINDOW_NORMAL)\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333d1afb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_insight(total_time, mean_posture, mean_face, mean_total, client_scores):\n",
        "    prompt = f\"\"\"\n",
        "Customer stayed for {total_time:.1f} seconds.\n",
        "Mean posture score: {mean_posture:.1f}/100\n",
        "Mean face score: {mean_face:.1f}/100\n",
        "Total mean satisfaction: {mean_total:.1f}/100\n",
        "Scores over time (posture, face, total): {client_scores}\n",
        "\n",
        "Give a short, professional insight (1-2 sentences) about possible customer mood or service issue.\n",
        "Focus on trends in posture (e.g. crossed arms = bored/impatient) and face expressions.\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3-8b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a customer service analyst. Be concise, professional, and insightful.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def generate_insight_thread(total_time, mean_posture, mean_face, mean_total, client_scores):\n",
        "    insight = generate_insight(total_time, mean_posture, mean_face, mean_total, client_scores)\n",
        "    print(\"Insight:\", insight)\n",
        "    global insight_text, insight_start\n",
        "    insight_text = insight\n",
        "    insight_start = time.monotonic()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insight_thread = None\n",
        "last_body_frame = None\n",
        "esc_pressed = False\n",
        "graph_height = 200\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "\n",
        "    fh, fw = frame.shape[:2]\n",
        "\n",
        "    results  = model.track(frame, persist=True, classes=[0], verbose=False)\n",
        "    annotated = frame.copy()\n",
        "    now = time.monotonic()\n",
        "\n",
        "    active_tracks_in_box = []\n",
        "    for box in results[0].boxes:\n",
        "        if box.id is None: continue\n",
        "        tid = int(box.id.item())\n",
        "        x1,y1,x2,y2 = map(int, box.xyxy[0].tolist())\n",
        "\n",
        "        overlap = box_overlap((x1,y1,x2,y2), client_box)\n",
        "        if overlap >= LEAVING_THRESHOLD:\n",
        "            active_tracks_in_box.append((tid, overlap, (x1,y1,x2,y2)))\n",
        "\n",
        "    front_tid = None\n",
        "    if active_tracks_in_box:\n",
        "        high_overlap_candidates = [t for t in active_tracks_in_box if t[1] >= ENTERING_THRESHOLD]\n",
        "        current_overlap = next((ov for tid,ov,_ in active_tracks_in_box if tid == current_client_tid), 0.0)\n",
        "        if current_overlap > 0:\n",
        "            front_tid = current_client_tid\n",
        "        elif high_overlap_candidates:\n",
        "            high_overlap_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "            front_tid = high_overlap_candidates[0][0]\n",
        "\n",
        "    if front_tid is not None:\n",
        "        if current_client_tid is None or current_client_tid != front_tid:\n",
        "            if current_client_tid is not None:\n",
        "                total_time = now - client_start_time\n",
        "                if client_scores:\n",
        "                    posture_scores = [s[0] for s in client_scores]\n",
        "                    face_scores    = [s[1] for s in client_scores]\n",
        "                    agg_scores     = [s[2] for s in client_scores]\n",
        "                    mean_posture   = np.mean(posture_scores)\n",
        "                    mean_face      = np.mean(face_scores)\n",
        "                    mean_total     = np.mean(agg_scores)\n",
        "                else:\n",
        "                    mean_posture = mean_face = mean_total = 0\n",
        "\n",
        "                print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left.\")\n",
        "                print(f\"Time spent: {total_time:.1f}s\")\n",
        "                print(f\"Scores list: {client_scores}\")\n",
        "                print(f\"Mean posture: {mean_posture:.1f}\")\n",
        "                print(f\"Mean face: {mean_face:.1f}\")\n",
        "                print(f\"Total mean score: {mean_total:.1f}\")\n",
        "                client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
        "\n",
        "                threading.Thread(target=generate_insight_thread, args=(total_time, mean_posture, mean_face, mean_total, client_scores)).start()\n",
        "\n",
        "            current_client_tid = front_tid\n",
        "            client_start_time  = now\n",
        "            client_scores      = []\n",
        "            last_score_time    = now\n",
        "            agg_history = []\n",
        "            insight_text = \"\"\n",
        "            print(f\"[{time.strftime('%H:%M:%S')}] New client ID {front_tid} entered.\")\n",
        "\n",
        "        tid = current_client_tid\n",
        "        if tid not in landmark_histories:\n",
        "            landmark_histories[tid]       = collections.deque(maxlen=HISTORY_LEN)\n",
        "            torso_activity_histories[tid] = collections.deque(maxlen=5)\n",
        "            prev_body_scores[tid]         = None\n",
        "            prev_face_scores[tid]         = None\n",
        "\n",
        "        person_bbox = next(b for t,o,b in active_tracks_in_box if t == tid)\n",
        "        x1, y1, x2, y2 = person_bbox\n",
        "\n",
        "        has_pose = False\n",
        "        pts = None\n",
        "        if results[0].keypoints is not None and len(results[0].keypoints) > 0:\n",
        "            for i, kp in enumerate(results[0].keypoints):\n",
        "                if results[0].boxes[i].id is not None and int(results[0].boxes[i].id.item()) == tid:\n",
        "                    keypoints = kp.data.cpu().numpy()\n",
        "                    pts = keypoints[0, :, :2]\n",
        "                    confidences = keypoints[0, :, 2]\n",
        "                    low_conf_mask = confidences < 0.5\n",
        "                    pts[low_conf_mask] = [0, 0]\n",
        "                    pts[:, 0] /= fw\n",
        "                    pts[:, 1] /= fh\n",
        "                    has_pose = True\n",
        "                    for idx1, idx2 in COCO_CONNECTIONS:\n",
        "                        if np.all(pts[idx1] != 0) and np.all(pts[idx2] != 0):\n",
        "                            pt1 = (int(pts[idx1][0] * fw), int(pts[idx1][1] * fh))\n",
        "                            pt2 = (int(pts[idx2][0] * fw), int(pts[idx2][1] * fh))\n",
        "                            cv2.line(annotated, pt1, pt2, (255, 0, 0), 2)\n",
        "                    break\n",
        "\n",
        "        face_box, face_crop = get_face_for_person(person_bbox, now)\n",
        "\n",
        "        if face_box:\n",
        "            fx1, fy1, fx2, fy2 = face_box\n",
        "            cv2.rectangle(annotated, (fx1, fy1), (fx2, fy2), (0, 255, 255), 2)\n",
        "\n",
        "        cv2.rectangle(annotated, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(annotated, f\"ID:{tid}\", (x1, y1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
        "\n",
        "        if has_pose and pts is not None and pts.shape[0] >= 17:\n",
        "            landmark_histories[tid].append(pts)\n",
        "\n",
        "        if now - last_score_time >= SCORE_INTERVAL:\n",
        "            last_score_time = now\n",
        "\n",
        "            body_sc = 0\n",
        "            if tid in landmark_histories and len(landmark_histories[tid]) >= 3:\n",
        "                _, body_sc, _ = compute_satisfaction_score(\n",
        "                    landmark_histories[tid], fw, fh,\n",
        "                    prev_body_scores.get(tid))\n",
        "                prev_body_scores[tid] = body_sc\n",
        "\n",
        "            face_sc = 0\n",
        "            if face_crop is not None:\n",
        "                _, face_sc, _ = compute_face_expression_score(\n",
        "                    face_crop, \n",
        "                    prev_face_scores.get(tid))\n",
        "                prev_face_scores[tid] = face_sc\n",
        "\n",
        "            scores = [s for s in (body_sc, face_sc) if s > 0]\n",
        "            agg_sc = sum(scores)/len(scores) if scores else 0\n",
        "\n",
        "            client_scores.append((body_sc, face_sc, agg_sc))\n",
        "            agg_history.append(agg_sc)\n",
        "\n",
        "            current_face_sc = face_sc\n",
        "            current_body_sc = body_sc\n",
        "            current_agg_sc = agg_sc\n",
        "\n",
        "    else:\n",
        "        if current_client_tid is not None:\n",
        "            total_time = now - client_start_time\n",
        "            if client_scores:\n",
        "                posture_scores = [s[0] for s in client_scores]\n",
        "                face_scores    = [s[1] for s in client_scores]\n",
        "                agg_scores     = [s[2] for s in client_scores]\n",
        "                mean_posture   = np.mean(posture_scores)\n",
        "                mean_face      = np.mean(face_scores)\n",
        "                mean_total     = np.mean(agg_scores)\n",
        "            else:\n",
        "                mean_posture = mean_face = mean_total = 0\n",
        "\n",
        "            print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left.\")\n",
        "            print(f\"Time spent: {total_time:.1f}s\")\n",
        "            print(f\"Scores list: {client_scores}\")\n",
        "            print(f\"Mean posture: {mean_posture:.1f}\")\n",
        "            print(f\"Mean face: {mean_face:.1f}\")\n",
        "            print(f\"Total mean score: {mean_total:.1f}\")\n",
        "            client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
        "\n",
        "            threading.Thread(target=generate_insight_thread, args=(total_time, mean_posture, mean_face, mean_total, client_scores)).start()\n",
        "\n",
        "            current_client_tid = None\n",
        "\n",
        "    if client_box:\n",
        "        cv2.rectangle(annotated, (client_box[0], client_box[1]), (client_box[2], client_box[3]), (255,0,0), 2)\n",
        "\n",
        "    cv2.putText(annotated, f\"face : {current_face_sc}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "    cv2.putText(annotated, f\"posture : {current_body_sc}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "    cv2.putText(annotated, f\"total : {current_agg_sc}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "    if current_client_tid is not None:\n",
        "        elapsed = now - client_start_time\n",
        "        chron_text = f\"Client Time: {elapsed:.1f}s\"\n",
        "        text_size = cv2.getTextSize(chron_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
        "        chron_x = fw - text_size[0] - 15\n",
        "        chron_y = 35\n",
        "        cv2.putText(\n",
        "            annotated,\n",
        "            chron_text,\n",
        "            (chron_x, chron_y),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.8,\n",
        "            (0, 255, 0),\n",
        "            2,\n",
        "            cv2.LINE_AA,\n",
        "        )\n",
        "    if monitoring_mode:\n",
        "        last_body_frame = annotated.copy()\n",
        "        canvas = np.full((graph_height + 120, annotated.shape[1], 3), 255, dtype=np.uint8)\n",
        "\n",
        "        if len(agg_history) > 0:\n",
        "            max_x = annotated.shape[1] - 100\n",
        "            max_y = graph_height - 100\n",
        "            points = []\n",
        "            for i, score in enumerate(agg_history):\n",
        "                x = 50 + int(i * max_x / max(1, len(agg_history) - 1))\n",
        "                y = (graph_height - 50) - int(score / 100 * max_y)\n",
        "                points.append((x, y))\n",
        "                cv2.circle(canvas, (x, y), 3, (0, 0, 255), -1)\n",
        "            for i in range(1, len(points)):\n",
        "                cv2.line(canvas, points[i-1], points[i], (0, 0, 255), 2)\n",
        "\n",
        "            cv2.line(canvas, (50, graph_height - 50), (annotated.shape[1] - 50, graph_height - 50), (0,0,0), 2)\n",
        "            cv2.line(canvas, (50, graph_height - 50), (50, 50), (0,0,0), 2)\n",
        "            cv2.putText(canvas, \"0\",   (30, graph_height - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "            cv2.putText(canvas, \"100\", (20, 60),               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "            cv2.putText(canvas, \"Satisfaction over time\", (annotated.shape[1]//2 - 100, 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2)\n",
        "        else:\n",
        "            cv2.putText(canvas, \"No data yet\", (10, graph_height // 2),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2)\n",
        "\n",
        "        if insight_text and now - insight_start < INSIGHT_DURATION:\n",
        "            max_line_w = annotated.shape[1] - 100\n",
        "            words = insight_text.split()\n",
        "            lines = []\n",
        "            cur = \"\"\n",
        "            for w in words:\n",
        "                test = cur + \" \" + w if cur else w\n",
        "                if cv2.getTextSize(test, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0][0] <= max_line_w:\n",
        "                    cur = test\n",
        "                else:\n",
        "                    lines.append(cur)\n",
        "                    cur = w\n",
        "            if cur:\n",
        "                lines.append(cur)\n",
        "\n",
        "            y0 = graph_height + 30\n",
        "            for i, line in enumerate(lines):\n",
        "                cv2.putText(canvas, line, (50, y0 + i*25),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 1, cv2.LINE_AA)\n",
        "\n",
        "        annotated = np.vstack((annotated, canvas))\n",
        "\n",
        "    cv2.imshow(\"YOLO + Pose + Face (Satisfaction)\", annotated)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == 27:\n",
        "        esc_pressed = True\n",
        "        insight_text = \"\"\n",
        "        break\n",
        "\n",
        "if current_client_tid is not None:\n",
        "    total_time = now - client_start_time\n",
        "    if client_scores:\n",
        "        posture_scores = [s[0] for s in client_scores]\n",
        "        face_scores    = [s[1] for s in client_scores]\n",
        "        agg_scores     = [s[2] for s in client_scores]\n",
        "        mean_posture   = np.mean(posture_scores)\n",
        "        mean_face      = np.mean(face_scores)\n",
        "        mean_total     = np.mean(agg_scores)\n",
        "    else:\n",
        "        mean_posture = mean_face = mean_total = 0\n",
        "\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] Client ID {current_client_tid} left (video end).\")\n",
        "\n",
        "    print(f\"Time spent: {total_time:.1f}s\")\n",
        "    print(f\"Scores list: {client_scores}\")\n",
        "    print(f\"Mean posture: {mean_posture:.1f}\")\n",
        "    print(f\"Mean face: {mean_face:.1f}\")\n",
        "    print(f\"Total mean score: {mean_total:.1f}\")\n",
        "    client_logs.append((current_client_tid, total_time, mean_posture, mean_face, mean_total))\n",
        "\n",
        "    if not esc_pressed:\n",
        "        insight_thread = threading.Thread(target=generate_insight_thread, args=(total_time, mean_posture, mean_face, mean_total, client_scores))\n",
        "        insight_thread.start()\n",
        "        insight_thread.join()\n",
        "\n",
        "if insight_thread is not None and last_body_frame is not None:\n",
        "    now = time.monotonic()\n",
        "    end_time = now + INSIGHT_DURATION\n",
        "    while now < end_time:\n",
        "        canvas = np.full((graph_height + 120, last_body_frame.shape[1], 3), 255, dtype=np.uint8)\n",
        "\n",
        "        if len(agg_history) > 0:\n",
        "            max_x = last_body_frame.shape[1] - 100\n",
        "            max_y = graph_height - 100\n",
        "            points = []\n",
        "            for i, score in enumerate(agg_history):\n",
        "                x = 50 + int(i * max_x / max(1, len(agg_history) - 1))\n",
        "                y = (graph_height - 50) - int(score / 100 * max_y)\n",
        "                points.append((x, y))\n",
        "                cv2.circle(canvas, (x, y), 3, (0, 0, 255), -1)\n",
        "            for i in range(1, len(points)):\n",
        "                cv2.line(canvas, points[i-1], points[i], (0, 0, 255), 2)\n",
        "\n",
        "            cv2.line(canvas, (50, graph_height - 50), (last_body_frame.shape[1] - 50, graph_height - 50), (0,0,0), 2)\n",
        "            cv2.line(canvas, (50, graph_height - 50), (50, 50), (0,0,0), 2)\n",
        "            cv2.putText(canvas, \"0\", (30, graph_height - 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "            cv2.putText(canvas, \"100\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
        "            cv2.putText(canvas, \"Satisfaction over time\", (last_body_frame.shape[1]//2 - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2)\n",
        "        else:\n",
        "            cv2.putText(canvas, \"No data yet\", (10, graph_height // 2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2)\n",
        "\n",
        "        if insight_text and now - insight_start < INSIGHT_DURATION:\n",
        "            max_line_w = last_body_frame.shape[1] - 100\n",
        "            words = insight_text.split()\n",
        "            lines = []\n",
        "            cur = \"\"\n",
        "            for w in words:\n",
        "                test = cur + \" \" + w if cur else w\n",
        "                if cv2.getTextSize(test, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0][0] <= max_line_w:\n",
        "                    cur = test\n",
        "                else:\n",
        "                    lines.append(cur)\n",
        "                    cur = w\n",
        "            if cur:\n",
        "                lines.append(cur)\n",
        "\n",
        "            y0 = graph_height + 30\n",
        "            for i, line in enumerate(lines):\n",
        "                cv2.putText(canvas, line, (50, y0 + i*25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 1, cv2.LINE_AA)\n",
        "\n",
        "        annotated = np.vstack((last_body_frame, canvas))\n",
        "        cv2.imshow(\"YOLO + Pose + Face (Satisfaction)\", annotated)\n",
        "        if cv2.waitKey(1) & 0xFF == 27:\n",
        "            break\n",
        "        time.sleep(0.01)\n",
        "        now = time.monotonic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa84aed",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"All client logs:\")\n",
        "for log in client_logs:\n",
        "    print(log)\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage\n",
        "1. Ensure the dependencies are installed and your `.env` file contains `OPENROUTER_API_KEY` (if using insight generation).\n",
        "2. Run the single long code cell below. It will open a window for selecting the client box and then run the monitoring loop.\n",
        "\n",
        "**To stop:** focus the video window and press `Esc`.\n",
        "\n",
        "If you need a split-up or modular notebook (cells for functions, downloads, and main loop), tell me and I'll create it."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
